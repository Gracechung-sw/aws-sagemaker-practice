{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f21e958",
   "metadata": {
    "papermill": {
     "duration": 0.021009,
     "end_time": "2022-04-18T00:13:14.040150",
     "exception": false,
     "start_time": "2022-04-18T00:13:14.019141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Amazon SageMaker Model Monitor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ec9d5c",
   "metadata": {
    "papermill": {
     "duration": 0.021009,
     "end_time": "2022-04-18T00:13:14.040150",
     "exception": false,
     "start_time": "2022-04-18T00:13:14.019141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook shows how to:\n",
    "\n",
    "- Amazon SageMaker에서 머신 러닝 모델을 호스팅하고 추론 요청, 결과 및 메타데이터를 캡처하는 방법\n",
    "- 훈련 데이터셋을 분석하여 기본 제약 조건을 생성하는 방법\n",
    "- 라이브 엔드포인트를 모니터링하여 제약 조건 위반을 감지하는 방법\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "Amazon SageMaker는 배포된 모델의 호출에 대한 입력, 출력 및 메타데이터를 캡처할 수 있게 해줍니다. 데이터를 분석하고 품질을 모니터링할 수 있는 기능도 제공합니다. 이에 대한 실습을 진행해보겠습니다.\n",
    "\n",
    "## Runtime\n",
    "\n",
    "This notebook uses an hourly monitor, so it takes between 30-90 minutes to run.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [파트 A: Amazon SageMaker 엔드포인트에서 실시간 추론 데이터 캡처](#PART-A:-Capturing-real-time-inference-data-from-Amazon-SageMaker-endpoints)\n",
    "1. [파트 B: 모델 모니터 - 기준 설정 및 지속적인 모니터링](#PART-B:-Model-Monitor---Baselining-and-continuous-monitoring)\n",
    "   1. [Constraint suggestion with baseline/training dataset](#1.-Constraint-suggestion-with-baseline/training-dataset)\n",
    "   1. [Analyze collected data for data quality issues](#2.-Analyze-collected-data-for-data-quality-issues)\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "To get started, make sure you have these prerequisites completed:\n",
    "\n",
    "- Specify an AWS Region to host your model.\n",
    "- An IAM role ARN exists that is used to give Amazon SageMaker access to your data in Amazon Simple Storage Service (Amazon S3).\n",
    "- Use the default S3 bucket to store the data used to train your model, any additional model data, and the data captured from model invocations. For demonstration purposes, you are using the same bucket for these. In reality, you might want to separate them with different security policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06059fc2",
   "metadata": {
    "isConfigCell": true,
    "papermill": {
     "duration": 1.760865,
     "end_time": "2022-04-18T00:13:15.822056",
     "exception": false,
     "start_time": "2022-04-18T00:13:14.061191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Role ARN: arn:aws:iam::672381912436:role/service-role/AmazonSageMaker-ExecutionRole-20240723T040833\n",
      "Demo Bucket: sagemaker-ap-northeast-2-672381912436\n",
      "Capture path: s3://sagemaker-ap-northeast-2-672381912436/sagemaker/DEMO-ModelMonitor/datacapture\n",
      "Report path: s3://sagemaker-ap-northeast-2-672381912436/sagemaker/DEMO-ModelMonitor/reports\n",
      "Preproc Code path: s3://sagemaker-ap-northeast-2-672381912436/sagemaker/DEMO-ModelMonitor/code/preprocessor.py\n",
      "Postproc Code path: s3://sagemaker-ap-northeast-2-672381912436/sagemaker/DEMO-ModelMonitor/code/postprocessor.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role, session\n",
    "\n",
    "sm_session = sagemaker.Session()\n",
    "region = sm_session.boto_region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"Role ARN: {}\".format(role))\n",
    "\n",
    "bucket = sm_session.default_bucket()\n",
    "print(\"Demo Bucket: {}\".format(bucket))\n",
    "prefix = \"sagemaker/DEMO-ModelMonitor\"\n",
    "\n",
    "data_capture_prefix = \"{}/datacapture\".format(prefix)\n",
    "s3_capture_upload_path = \"s3://{}/{}\".format(bucket, data_capture_prefix)\n",
    "reports_prefix = \"{}/reports\".format(prefix)\n",
    "s3_report_path = \"s3://{}/{}\".format(bucket, reports_prefix)\n",
    "code_prefix = \"{}/code\".format(prefix)\n",
    "s3_code_preprocessor_uri = \"s3://{}/{}/{}\".format(bucket, code_prefix, \"preprocessor.py\")\n",
    "s3_code_postprocessor_uri = \"s3://{}/{}/{}\".format(bucket, code_prefix, \"postprocessor.py\")\n",
    "\n",
    "print(\"Capture path: {}\".format(s3_capture_upload_path))\n",
    "print(\"Report path: {}\".format(s3_report_path))\n",
    "print(\"Preproc Code path: {}\".format(s3_code_preprocessor_uri))\n",
    "print(\"Postproc Code path: {}\".format(s3_code_postprocessor_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45e9a54",
   "metadata": {
    "papermill": {
     "duration": 0.021449,
     "end_time": "2022-04-18T00:13:15.865605",
     "exception": false,
     "start_time": "2022-04-18T00:13:15.844156",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PART A: Capturing real-time inference data from Amazon SageMaker endpoints\n",
    "\n",
    "Create an endpoint to showcase the data capture capability in action.\n",
    "\n",
    "### Upload the pre-trained model to Amazon S3\n",
    "\n",
    "이 코드는 배포할 준비가 된 사전 훈련된 XGBoost 모델을 업로드합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee26d288",
   "metadata": {
    "papermill": {
     "duration": 0.16406,
     "end_time": "2022-04-18T00:13:16.051105",
     "exception": false,
     "start_time": "2022-04-18T00:13:15.887045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_file = open(\"model/xgb-churn-prediction-model.tar.gz\", \"rb\")\n",
    "s3_key = os.path.join(prefix, \"xgb-churn-prediction-model.tar.gz\")\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(s3_key).upload_fileobj(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad4a66",
   "metadata": {
    "papermill": {
     "duration": 0.021593,
     "end_time": "2022-04-18T00:13:16.094614",
     "exception": false,
     "start_time": "2022-04-18T00:13:16.073021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Deploy the model to Amazon SageMaker\n",
    "\n",
    "Start with deploying a pre-trained churn prediction model. Here, you create the model object with the image and model data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ad19f39",
   "metadata": {
    "papermill": {
     "duration": 0.040689,
     "end_time": "2022-04-18T00:13:16.156984",
     "exception": false,
     "start_time": "2022-04-18T00:13:16.116295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "model_name = \"DEMO-xgb-churn-pred-model-monitor-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "model_url = \"https://{}.s3-{}.amazonaws.com/{}/xgb-churn-prediction-model.tar.gz\".format(\n",
    "    bucket, region, prefix\n",
    ")\n",
    "\n",
    "image_uri = retrieve(\"xgboost\", region, \"0.90-1\")\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=model_url, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7b61e",
   "metadata": {
    "papermill": {
     "duration": 0.02197,
     "end_time": "2022-04-18T00:13:16.200979",
     "exception": false,
     "start_time": "2022-04-18T00:13:16.179009",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "모델 데이터 품질을 모니터링하기 위해 데이터 캡처를 활성화하려면 DataCaptureConfig라는 새로운 캡처 옵션을 지정합니다.\n",
    "이를 통해서 요청이나 응답에 대한 페이로드를 캡쳐할 수 있습니다.\n",
    "이후 deployment 단계에서 해봤던 것과 동일하게 .deploy를 사용하려 endpoint 배포를 진행해봅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a8f85c2",
   "metadata": {
    "papermill": {
     "duration": 452.17232,
     "end_time": "2022-04-18T00:20:48.395194",
     "exception": false,
     "start_time": "2022-04-18T00:13:16.222874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndpointName=DEMO-xgb-churn-pred-model-monitor-2024-07-22-20-26-07\n",
      "-----!"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "endpoint_name = \"DEMO-xgb-churn-pred-model-monitor-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"EndpointName={}\".format(endpoint_name))\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True, sampling_percentage=100, destination_s3_uri=s3_capture_upload_path\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    data_capture_config=data_capture_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ed7979",
   "metadata": {
    "papermill": {
     "duration": 0.025376,
     "end_time": "2022-04-18T00:20:48.446074",
     "exception": false,
     "start_time": "2022-04-18T00:20:48.420698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Invoke the deployed model\n",
    "\n",
    "deploy가 완료되었다면 생성된 엔드포인트에 데이터를 보내서 실시간으로 추론을 할 수 있습니다.\n",
    "이전 단계에서 데이터 캡처를 활성화했기 때문에 요청 및 응답 페이로드와 추가 메타데이터가 DataCaptureConfig에서 지정한 s3위치에 저장되게 됩니다.\n",
    "\n",
    "이 단계에서는 약 3분 동안 포함된 샘플 데이터를 사용하여 엔드포인트를 호출해서 데이터가 캡쳐되도록 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e848dd92",
   "metadata": {
    "papermill": {
     "duration": 0.025161,
     "end_time": "2022-04-18T00:20:48.496491",
     "exception": false,
     "start_time": "2022-04-18T00:20:48.471330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This step invokes the endpoint with included sample data for about 3 minutes. Data is captured based on the sampling percentage specified and the capture continues until the data capture option is turned off.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d376f93",
   "metadata": {
    "papermill": {
     "duration": 183.152276,
     "end_time": "2022-04-18T00:23:51.674166",
     "exception": false,
     "start_time": "2022-04-18T00:20:48.521890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending test traffic to the endpoint DEMO-xgb-churn-pred-model-monitor-2024-07-22-20-26-07. \n",
      "Please wait...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "import time\n",
    "\n",
    "predictor = Predictor(endpoint_name=endpoint_name, serializer=CSVSerializer())\n",
    "\n",
    "# Get a subset of test data for a quick test\n",
    "!head -180 test_data/test-dataset-input-cols.csv > test_data/test_sample.csv\n",
    "print(\"Sending test traffic to the endpoint {}. \\nPlease wait...\".format(endpoint_name))\n",
    "\n",
    "with open(\"test_data/test_sample.csv\", \"r\") as f:\n",
    "    for row in f:\n",
    "        payload = row.rstrip(\"\\n\")\n",
    "        response = predictor.predict(data=payload)\n",
    "        time.sleep(1)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12963ac1",
   "metadata": {
    "papermill": {
     "duration": 0.025961,
     "end_time": "2022-04-18T00:23:51.726250",
     "exception": false,
     "start_time": "2022-04-18T00:23:51.700289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### View captured data\n",
    "\n",
    "Amazon S3에 저장된 데이터 캡처 파일이 어떤게 있는지 가져와보겠습니다.\n",
    "\n",
    "`s3://{destination-bucket-prefix}/{endpoint-name}/{variant-name}/yyyy/mm/dd/hh/filename.jsonl`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be89b97b",
   "metadata": {
    "papermill": {
     "duration": 0.213801,
     "end_time": "2022-04-18T00:23:51.966129",
     "exception": false,
     "start_time": "2022-04-18T00:23:51.752328",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Capture Files:\n",
      "sagemaker/DEMO-ModelMonitor/datacapture/DEMO-xgb-churn-pred-model-monitor-2024-07-22-20-26-07/AllTraffic/2024/07/22/20/29-10-429-96d4b620-b7f7-4921-9a1b-72800e5b36ee.jsonl\n",
      " sagemaker/DEMO-ModelMonitor/datacapture/DEMO-xgb-churn-pred-model-monitor-2024-07-22-20-26-07/AllTraffic/2024/07/22/20/30-10-794-c32f4e48-3cac-4e46-a62b-3b47de95bc10.jsonl\n",
      " sagemaker/DEMO-ModelMonitor/datacapture/DEMO-xgb-churn-pred-model-monitor-2024-07-22-20-26-07/AllTraffic/2024/07/22/20/31-10-904-96a29de4-ba16-49c4-962d-5dbc41e67761.jsonl\n",
      " sagemaker/DEMO-ModelMonitor/datacapture/DEMO-xgb-churn-pred-model-monitor-2024-07-22-20-26-07/AllTraffic/2024/07/22/20/32-10-982-0ad712d4-9cf8-4ba6-b0c0-3df01dc69b2c.jsonl\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.Session().client(\"s3\")\n",
    "current_endpoint_capture_prefix = \"{}/{}\".format(data_capture_prefix, endpoint_name)\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get(\"Contents\")]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b147b62",
   "metadata": {
    "papermill": {
     "duration": 0.027531,
     "end_time": "2022-04-18T00:23:52.029034",
     "exception": false,
     "start_time": "2022-04-18T00:23:52.001503",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "한 파일을 열어서 JSON-line형식으로 캡쳐된 데이터를 한 번 출력해봅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16f20ef8",
   "metadata": {
    "papermill": {
     "duration": 0.118865,
     "end_time": "2022-04-18T00:23:52.174189",
     "exception": false,
     "start_time": "2022-04-18T00:23:52.055324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"108,0,198.5,99,267.8,60,354.9,75,9.4,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,1,0\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"0.03810077905654907\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"6caf6eee-822f-41fe-8506-26c999b32ce2\",\"inferenceTime\":\"2024-07-22T20:32:10Z\"},\"eventVersion\":\"0\"}\n",
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"71,0,103.3,103,138.5,79,164.8,98,9.0,2,2,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"0.03876146301627159\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"9d73d8e1-9ffa-46d0-8769-58f322542816\",\"inferenceTime\":\"2024-07-22T20:32:11Z\"},\"eventVersion\":\"0\"}\n",
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"113,32,180.4,89,129.4,124,166.9,124,8.4,2,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"0.04491584002971649\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"4e729d47-8699-41b7-a777-03cfb4ba4e9b\",\"inferenceTime\":\"2024-07-22T20:32:13Z\"},\"eventVersion\":\"0\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get(\"Body\").read().decode(\"utf-8\")\n",
    "\n",
    "\n",
    "capture_file = get_obj_body(capture_files[-1])\n",
    "print(capture_file[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427cf179",
   "metadata": {
    "papermill": {
     "duration": 0.026714,
     "end_time": "2022-04-18T00:23:52.227979",
     "exception": false,
     "start_time": "2022-04-18T00:23:52.201265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "아래의 코드를 실행해서 좀 더 보기쉬운 형태로 확인해보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edbca102",
   "metadata": {
    "papermill": {
     "duration": 0.034824,
     "end_time": "2022-04-18T00:23:52.289438",
     "exception": false,
     "start_time": "2022-04-18T00:23:52.254614",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"captureData\": {\n",
      "    \"endpointInput\": {\n",
      "      \"observedContentType\": \"text/csv\",\n",
      "      \"mode\": \"INPUT\",\n",
      "      \"data\": \"108,0,198.5,99,267.8,60,354.9,75,9.4,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,1,0\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    },\n",
      "    \"endpointOutput\": {\n",
      "      \"observedContentType\": \"text/csv; charset=utf-8\",\n",
      "      \"mode\": \"OUTPUT\",\n",
      "      \"data\": \"0.03810077905654907\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    }\n",
      "  },\n",
      "  \"eventMetadata\": {\n",
      "    \"eventId\": \"6caf6eee-822f-41fe-8506-26c999b32ce2\",\n",
      "    \"inferenceTime\": \"2024-07-22T20:32:10Z\"\n",
      "  },\n",
      "  \"eventVersion\": \"0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(json.loads(capture_file.split(\"\\n\")[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b6245",
   "metadata": {
    "papermill": {
     "duration": 0.026948,
     "end_time": "2022-04-18T00:23:52.343438",
     "exception": false,
     "start_time": "2022-04-18T00:23:52.316490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "다음으로는 Amazon SageMaker가 Amazon S3에서 수집된 데이터를 모니터링하는 방법에 대해 보겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e43e3",
   "metadata": {
    "papermill": {
     "duration": 0.02743,
     "end_time": "2022-04-18T00:23:52.397993",
     "exception": false,
     "start_time": "2022-04-18T00:23:52.370563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PART B: Model Monitor - Baselining and continuous monitoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbd7dfd",
   "metadata": {
    "papermill": {
     "duration": 0.026767,
     "end_time": "2022-04-18T00:23:52.452009",
     "exception": false,
     "start_time": "2022-04-18T00:23:52.425242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "데이터 수집 외에도, Amazon SageMaker는 엔드포인트에서 관찰된 데이터를 모니터링하고 평가할 수 있는 기능을 제공합니다. 이를 위해 다음 단계를 따릅니다:\n",
    "\n",
    "1. 실시간 트래픽과 비교할 기준선을 설정합니다.\n",
    "2. 기준선이 준비되면, 기준선과 비교할 수 있도록 지속적으로 평가하고 모니터링할 일정을 설정합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377e8855",
   "metadata": {
    "papermill": {
     "duration": 0.026868,
     "end_time": "2022-04-18T00:23:52.505930",
     "exception": false,
     "start_time": "2022-04-18T00:23:52.479062",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. Constraint suggestion with baseline/training dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c596ed4c",
   "metadata": {
    "papermill": {
     "duration": 0.026955,
     "end_time": "2022-04-18T00:23:52.559834",
     "exception": false,
     "start_time": "2022-04-18T00:23:52.532879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The training dataset with which you trained the model is usually a good baseline dataset. Note that the training dataset data schema and the inference dataset schema should exactly match (i.e. the number and order of the features).\n",
    "\n",
    "From the training dataset you can ask Amazon SageMaker to suggest a set of baseline `constraints` and generate descriptive `statistics` to explore the data. For this example, upload the training dataset that was used to train the pre-trained model included in this example. If you already have it in Amazon S3, you can directly point to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dce35f31",
   "metadata": {
    "papermill": {
     "duration": 0.034674,
     "end_time": "2022-04-18T00:23:52.621397",
     "exception": false,
     "start_time": "2022-04-18T00:23:52.586723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline data uri: s3://sagemaker-ap-northeast-2-672381912436/sagemaker/DEMO-ModelMonitor/baselining/data\n",
      "Baseline results uri: s3://sagemaker-ap-northeast-2-672381912436/sagemaker/DEMO-ModelMonitor/baselining/results\n"
     ]
    }
   ],
   "source": [
    "# copy over the training dataset to Amazon S3 (if you already have it in Amazon S3, you could reuse it)\n",
    "baseline_prefix = prefix + \"/baselining\"\n",
    "baseline_data_prefix = baseline_prefix + \"/data\"\n",
    "baseline_results_prefix = baseline_prefix + \"/results\"\n",
    "\n",
    "baseline_data_uri = \"s3://{}/{}\".format(bucket, baseline_data_prefix)\n",
    "baseline_results_uri = \"s3://{}/{}\".format(bucket, baseline_results_prefix)\n",
    "print(\"Baseline data uri: {}\".format(baseline_data_uri))\n",
    "print(\"Baseline results uri: {}\".format(baseline_results_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db1f8aca",
   "metadata": {
    "papermill": {
     "duration": 0.212278,
     "end_time": "2022-04-18T00:23:52.861105",
     "exception": false,
     "start_time": "2022-04-18T00:23:52.648827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data_file = open(\"test_data/training-dataset-with-header.csv\", \"rb\")\n",
    "s3_key = os.path.join(baseline_prefix, \"data\", \"training-dataset-with-header.csv\")\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(s3_key).upload_fileobj(training_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3205c9d4",
   "metadata": {
    "papermill": {
     "duration": 0.03685,
     "end_time": "2022-04-18T00:23:52.925824",
     "exception": false,
     "start_time": "2022-04-18T00:23:52.888974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Create a baselining job with training dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7751107f",
   "metadata": {
    "papermill": {
     "duration": 0.028188,
     "end_time": "2022-04-18T00:23:52.997676",
     "exception": false,
     "start_time": "2022-04-18T00:23:52.969488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now that you have the training data ready in Amazon S3, start a job to `suggest` constraints. `DefaultModelMonitor.suggest_baseline(..)` starts a `ProcessingJob` using an Amazon SageMaker provided Model Monitor container to generate the constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63dc14e7",
   "metadata": {
    "papermill": {
     "duration": 353.320851,
     "end_time": "2022-04-18T00:29:46.346647",
     "exception": false,
     "start_time": "2022-04-18T00:23:53.025796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2024-07-22-20-46-52-342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............\u001b[34m2024-07-22 20:48:49.969461: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:49.969491: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:51.537862: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:51.537894: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:51.537919: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-117-124.ap-northeast-2.compute.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:51.538180: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:53,104 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:ap-northeast-2:672381912436:processing-job/baseline-suggestion-job-2024-07-22-20-46-52-342', 'ProcessingJobName': 'baseline-suggestion-job-2024-07-22-20-46-52-342', 'Environment': {'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '709848358524.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-ap-northeast-2-672381912436/sagemaker/DEMO-ModelMonitor/baselining/data/training-dataset-with-header.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-ap-northeast-2-672381912436/sagemaker/DEMO-ModelMonitor/baselining/results', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::672381912436:role/service-role/AmazonSageMaker-ExecutionRole-20240723T040833', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:53,104 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:53,104 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:53,105 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:53,105 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:53,105 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:53,150 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:53,151 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:53,151 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0'}\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:53,158 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:53,158 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:53,158 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:53,623 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.117.124\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-\u001b[0m\n",
      "\u001b[34mnodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_392\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:53,631 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:53,634 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-4407802b-9ee6-44a9-9ef6-7e6eb56c5d2f\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,160 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,171 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,172 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,174 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,178 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,178 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,178 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,178 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,208 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,219 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,219 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,223 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,226 INFO blockmanagement.BlockManager: The block deletion will start around 2024 Jul 22 20:48:54\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,227 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,227 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,229 INFO util.GSet: 2.0% max memory 3.0 GB = 62.0 MB\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,229 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,264 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,268 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,268 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,268 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,268 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,268 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,268 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,268 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,268 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,268 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,268 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,268 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,296 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,296 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,296 INFO util.GSet: 1.0% max memory 3.0 GB = 31.0 MB\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,296 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,298 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,298 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,298 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,298 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,302 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,306 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,306 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,306 INFO util.GSet: 0.25% max memory 3.0 GB = 7.7 MB\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,306 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,340 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,340 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,341 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,343 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,344 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,345 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,345 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,345 INFO util.GSet: 0.029999999329447746% max memory 3.0 GB = 952.0 KB\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,345 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,365 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1156786149-10.0.117.124-1721681334360\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,375 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,382 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,462 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,472 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,476 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.117.124\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:54,485 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:56,545 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:56,545 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:58,609 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2024-07-22 20:48:58,610 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:00,683 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:00,683 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:02,780 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:02,780 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:04,857 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:04,858 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:14,868 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:16,539 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:16,941 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:16,977 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:16,987 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:17,598 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:17,621 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:17,622 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:17,622 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:17,623 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:17,650 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11384, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:17,666 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:17,668 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:17,722 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:17,722 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:17,722 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:17,723 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:17,723 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:18,076 INFO util.Utils: Successfully started service 'sparkDriver' on port 34079.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:18,104 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:18,141 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:18,169 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:18,170 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:18,214 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:18,244 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-7367db3e-bdaf-4e87-b017-05481baf24a7\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:18,270 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:18,322 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:18,366 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.117.124:34079/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1721681357594\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:18,861 INFO client.RMProxy: Connecting to ResourceManager at /10.0.117.124:8032\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:19,534 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:19,534 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:19,540 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15524 MB per container)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:19,541 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:19,541 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:19,541 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:19,547 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:19,622 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:21,160 INFO yarn.Client: Uploading resource file:/tmp/spark-7940c544-7071-464f-845c-12d1fe581960/__spark_libs__3413083629345093484.zip -> hdfs://10.0.117.124/user/root/.sparkStaging/application_1721681339943_0001/__spark_libs__3413083629345093484.zip\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:22,257 INFO yarn.Client: Uploading resource file:/tmp/spark-7940c544-7071-464f-845c-12d1fe581960/__spark_conf__6133723361475478779.zip -> hdfs://10.0.117.124/user/root/.sparkStaging/application_1721681339943_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:22,702 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:22,702 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:22,702 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:22,702 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:22,702 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:22,728 INFO yarn.Client: Submitting application application_1721681339943_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:22,915 INFO impl.YarnClientImpl: Submitted application application_1721681339943_0001\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:23,919 INFO yarn.Client: Application report for application_1721681339943_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:23,922 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1721681362820\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1721681339943_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:24,925 INFO yarn.Client: Application report for application_1721681339943_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:25,928 INFO yarn.Client: Application report for application_1721681339943_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:26,932 INFO yarn.Client: Application report for application_1721681339943_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:27,921 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1721681339943_0001), /proxy/application_1721681339943_0001\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:27,936 INFO yarn.Client: Application report for application_1721681339943_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:27,936 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.117.124\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1721681362820\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1721681339943_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:27,937 INFO cluster.YarnClientSchedulerBackend: Application application_1721681339943_0001 has started running.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:27,948 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39417.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:27,949 INFO netty.NettyBlockTransferService: Server created on 10.0.117.124:39417\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:27,951 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:27,961 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.117.124, 39417, None)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:27,965 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.117.124:39417 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.117.124, 39417, None)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:27,969 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.117.124, 39417, None)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:27,971 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.117.124, 39417, None)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:28,132 INFO util.log: Logging initialized @13018ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:29,317 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:32,488 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.117.124:47078) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:32,672 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:33575 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 33575, None)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:48,792 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:49,003 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:49,050 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:49,055 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:50,089 INFO datasources.InMemoryFileIndex: It took 40 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:50,264 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:50,574 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:50,577 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.117.124:39417 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:50,582 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:50,940 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:50,942 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:50,945 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 375873\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:51,001 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:51,017 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:51,017 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:51,018 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:51,019 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:51,024 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:51,097 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:51,112 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:51,117 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.117.124:39417 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:51,120 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:51,140 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:51,142 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:51,197 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4641 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:51,446 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:33575 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:52,220 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:33575 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:52,579 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1397 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:52,581 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:52,587 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.542 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:52,600 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:52,600 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:52,602 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.601059 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:52,759 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.117.124:39417 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:52,777 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:33575 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:54,994 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:54,996 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:54,999 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Churn: string, Account Length: string, VMail Message: string, Day Mins: string, Day Calls: string ... 68 more fields>\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:55,045 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:55,258 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:55,270 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:55,271 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.117.124:39417 (size: 39.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:55,272 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:55,287 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:55,325 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:55,327 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:55,327 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:55,327 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:55,329 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:55,331 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:55,379 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 29.8 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:55,381 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 11.2 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:55,382 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.117.124:39417 (size: 11.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:55,382 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:55,383 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:55,383 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:55,387 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:55,428 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:33575 (size: 11.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:56,235 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:33575 (size: 39.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:56,548 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:33575 (size: 188.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:56,745 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1360 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:56,746 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:56,747 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 1.413 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:56,750 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:56,751 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:56,751 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 1.425542 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:57,245 INFO codegen.CodeGenerator: Code generated in 322.313633 ms\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:57,922 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:57,926 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:57,927 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:57,927 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:57,929 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:57,931 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:57,956 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 125.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:57,959 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 37.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:57,960 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.117.124:39417 (size: 37.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:57,961 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:57,962 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:57,962 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:57,971 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:57,999 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:33575 (size: 37.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:58,979 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1010 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:58,979 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:58,981 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.047 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:58,982 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:58,982 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:58,983 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:58,983 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,075 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,077 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,078 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,078 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,078 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,080 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,094 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,096 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,097 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.117.124:39417 (size: 49.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,097 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,098 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,098 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,100 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,115 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:33575 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,156 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,448 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 349 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,449 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,450 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.363 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,450 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,450 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,451 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.375582 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,490 INFO codegen.CodeGenerator: Code generated in 31.890637 ms\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,814 INFO codegen.CodeGenerator: Code generated in 36.855486 ms\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,931 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,933 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,933 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,933 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,934 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,937 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,948 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.117.124:39417 in memory (size: 37.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,954 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:33575 in memory (size: 37.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,985 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.117.124:39417 in memory (size: 49.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:49:59,987 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:33575 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,024 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.117.124:39417 in memory (size: 11.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,025 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:33575 in memory (size: 11.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,038 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 49.2 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,044 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 19.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,045 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.117.124:39417 (size: 19.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,045 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,046 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,046 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,049 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,076 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:33575 (size: 19.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,485 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 437 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,486 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 0.547 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,486 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,486 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,486 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,487 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 0.555136 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,958 INFO codegen.CodeGenerator: Code generated in 90.624564 ms\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,966 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,967 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,967 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,967 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,968 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,969 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,974 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 87.0 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,975 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,976 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.117.124:39417 (size: 27.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,976 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,977 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,977 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,978 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:00,995 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:33575 (size: 27.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,137 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 159 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,137 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,138 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.168 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,138 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,139 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,139 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,139 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,492 INFO codegen.CodeGenerator: Code generated in 200.801369 ms\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,508 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,511 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,511 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,511 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,512 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,512 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,520 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 67.4 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,523 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,529 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.117.124:39417 (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,529 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,530 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,530 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,532 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,552 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:33575 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,560 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,702 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 170 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,703 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,703 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.187 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,704 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,704 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,704 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.195499 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:01,851 INFO codegen.CodeGenerator: Code generated in 112.947353 ms\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:02,056 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:02,065 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:02,066 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:02,066 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:02,066 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:02,066 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:02,069 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:02,080 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 41.8 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:02,085 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:02,088 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.117.124:39417 (size: 17.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:02,089 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:02,089 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:02,089 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:02,091 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:02,111 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:33575 (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,697 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 1606 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,697 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,698 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 1.628 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,699 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,700 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,700 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,700 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,700 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,705 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,706 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,707 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.117.124:39417 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,708 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,708 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,708 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,710 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,726 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:33575 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,732 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,786 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 77 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,787 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,793 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.092 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,793 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,793 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:03,793 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 1.732101 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,033 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,034 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,034 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,034 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,034 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,035 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,046 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 94.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,048 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,049 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.117.124:39417 (size: 30.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,050 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,050 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,050 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,052 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,065 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:33575 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,209 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 156 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,209 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,210 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.169 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,210 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,210 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,211 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,211 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,253 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,254 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,254 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,255 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,255 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,256 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,263 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 179.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,265 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 49.3 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,266 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.117.124:39417 (size: 49.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,266 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,267 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,267 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,268 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,279 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:33575 (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,295 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,370 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 102 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,371 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,372 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.114 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,372 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,372 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,373 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.119664 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,515 INFO codegen.CodeGenerator: Code generated in 24.420915 ms\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,547 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,549 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,549 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,549 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,550 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,551 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,558 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 49.3 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,561 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,561 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.117.124:39417 (size: 19.8 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,562 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,562 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,562 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,564 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,584 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:33575 (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,692 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 128 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,692 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,693 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.141 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,694 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,694 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,694 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.147050 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,787 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:33575 in memory (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,791 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.117.124:39417 in memory (size: 19.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,803 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.117.124:39417 in memory (size: 17.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,808 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:33575 in memory (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,822 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.117.124:39417 in memory (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,827 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:33575 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,850 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:33575 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,851 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.117.124:39417 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,860 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.117.124:39417 in memory (size: 19.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,875 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:33575 in memory (size: 19.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,881 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.117.124:39417 in memory (size: 49.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,883 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:33575 in memory (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,889 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.117.124:39417 in memory (size: 27.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,899 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:33575 in memory (size: 27.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,903 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.117.124:39417 in memory (size: 30.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,904 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:33575 in memory (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,972 INFO codegen.CodeGenerator: Code generated in 46.767692 ms\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,979 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,979 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,980 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,980 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,981 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,982 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,986 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 86.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,987 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,988 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.117.124:39417 (size: 27.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,988 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,989 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,989 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:04,990 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,000 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:33575 (size: 27.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,071 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 81 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,071 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,073 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.090 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,073 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,073 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,073 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,073 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,277 INFO codegen.CodeGenerator: Code generated in 125.505849 ms\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,293 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,294 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,295 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,296 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,296 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,297 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,300 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 66.8 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,302 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,303 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.117.124:39417 (size: 19.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,304 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,304 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,304 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,306 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,316 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:33575 (size: 19.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,321 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,424 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 119 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,425 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,426 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.127 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,427 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,427 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,427 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.133817 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,489 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,491 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,492 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,492 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,492 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,492 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,495 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,509 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 41.8 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,514 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,515 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.117.124:39417 (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,515 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,516 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,516 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,517 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,537 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:33575 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,622 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 105 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,622 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,623 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.127 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,623 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,623 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,623 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,624 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,624 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,630 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,631 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,632 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.117.124:39417 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,632 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,633 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,633 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,635 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,651 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:33575 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,657 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,683 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 49 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,683 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,685 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.058 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,685 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,685 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,686 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.196403 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,881 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,881 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,881 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,881 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,882 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,883 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,887 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 94.7 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,889 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,889 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.117.124:39417 (size: 30.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,895 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,895 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,895 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,896 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:05,910 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:33575 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,094 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 198 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,094 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,095 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.212 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,096 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,096 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,096 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,096 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,143 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,146 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,147 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,147 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,147 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,148 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,157 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 179.6 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,159 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 49.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,160 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.117.124:39417 (size: 49.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,160 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,161 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,161 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,162 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,190 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:33575 (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,199 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,325 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 163 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,325 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,326 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.175 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,326 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,327 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,327 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.182697 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,516 INFO codegen.CodeGenerator: Code generated in 12.805512 ms\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,554 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,555 INFO scheduler.DAGScheduler: Got job 16 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,555 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,555 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,556 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,556 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,563 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 48.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,565 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,565 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.117.124:39417 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,566 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,566 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,566 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,567 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,578 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:33575 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,634 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 67 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,634 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,635 INFO scheduler.DAGScheduler: ResultStage 23 (treeReduce at KLLRunner.scala:107) finished in 0.078 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,636 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,636 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,636 INFO scheduler.DAGScheduler: Job 16 finished: treeReduce at KLLRunner.scala:107, took 0.082098 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,808 INFO codegen.CodeGenerator: Code generated in 33.280445 ms\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,814 INFO scheduler.DAGScheduler: Registering RDD 104 (collect at AnalysisRunner.scala:326) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,815 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,815 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,815 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,815 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,815 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,819 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 87.4 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,820 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,821 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.117.124:39417 (size: 27.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,821 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,824 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,824 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,825 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,836 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:33575 (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,924 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 99 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,924 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,925 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326) finished in 0.109 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,926 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,926 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,927 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:06,927 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,116 INFO codegen.CodeGenerator: Code generated in 83.02511 ms\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,129 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,130 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,131 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,131 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,131 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,132 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,137 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 67.7 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,142 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,143 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.117.124:39417 (size: 19.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,144 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,144 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,144 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,146 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,159 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:33575 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,166 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,283 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 138 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,286 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,294 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:33575 in memory (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,300 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:326) finished in 0.165 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,300 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,300 INFO cluster.YarnScheduler: Killing all running tasks in stage 26: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,301 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:326, took 0.171496 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,304 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.117.124:39417 in memory (size: 30.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,360 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,361 INFO scheduler.DAGScheduler: Registering RDD 115 (countByKey at ColumnProfiler.scala:592) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,361 INFO scheduler.DAGScheduler: Got job 19 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,362 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,362 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,362 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,363 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,391 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 41.8 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,394 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,395 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.117.124:39417 (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,396 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,396 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,397 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,398 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,409 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.117.124:39417 in memory (size: 27.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,421 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:33575 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,428 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:33575 in memory (size: 27.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,453 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.117.124:39417 in memory (size: 17.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,456 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:33575 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,486 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 88 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,488 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,489 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (countByKey at ColumnProfiler.scala:592) finished in 0.125 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,489 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,489 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,489 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 28)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,489 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,489 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,491 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 5.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,494 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,496 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.117.124:39417 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,497 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,498 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,498 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,501 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,506 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.117.124:39417 in memory (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,517 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:33575 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,518 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:33575 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,521 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,526 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.117.124:39417 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,527 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:33575 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,541 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 40 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,541 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,542 INFO scheduler.DAGScheduler: ResultStage 28 (countByKey at ColumnProfiler.scala:592) finished in 0.052 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,542 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,542 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,542 INFO scheduler.DAGScheduler: Job 19 finished: countByKey at ColumnProfiler.scala:592, took 0.182416 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,556 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.117.124:39417 in memory (size: 27.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,560 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:33575 in memory (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,565 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.117.124:39417 in memory (size: 19.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,568 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:33575 in memory (size: 19.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,573 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:33575 in memory (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,588 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.117.124:39417 in memory (size: 49.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,707 INFO scheduler.DAGScheduler: Registering RDD 121 (collect at AnalysisRunner.scala:326) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,707 INFO scheduler.DAGScheduler: Got map stage job 20 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,708 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,708 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,708 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,708 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,713 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 94.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,715 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,715 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.117.124:39417 (size: 30.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,716 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,716 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,717 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,718 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,727 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:33575 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,872 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 154 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,872 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,873 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326) finished in 0.163 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,873 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,873 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,873 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,873 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,902 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,903 INFO scheduler.DAGScheduler: Got job 21 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,903 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,903 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,903 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,904 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,908 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 179.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,910 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,910 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.117.124:39417 (size: 49.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,911 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,911 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,911 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,912 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,920 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:33575 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:07,930 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,050 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 138 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,051 INFO scheduler.DAGScheduler: ResultStage 31 (collect at AnalysisRunner.scala:326) finished in 0.147 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,051 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,052 INFO scheduler.DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,052 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,052 INFO scheduler.DAGScheduler: Job 21 finished: collect at AnalysisRunner.scala:326, took 0.149567 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,133 INFO codegen.CodeGenerator: Code generated in 9.817184 ms\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,164 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,165 INFO scheduler.DAGScheduler: Got job 22 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,165 INFO scheduler.DAGScheduler: Final stage: ResultStage 32 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,165 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,165 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,165 INFO scheduler.DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,175 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 48.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,176 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,177 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.117.124:39417 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,177 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,178 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,178 INFO cluster.YarnScheduler: Adding task set 32.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,179 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 25) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,188 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:33575 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,287 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 25) in 108 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,287 INFO cluster.YarnScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,288 INFO scheduler.DAGScheduler: ResultStage 32 (treeReduce at KLLRunner.scala:107) finished in 0.122 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,289 INFO scheduler.DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,289 INFO cluster.YarnScheduler: Killing all running tasks in stage 32: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,289 INFO scheduler.DAGScheduler: Job 22 finished: treeReduce at KLLRunner.scala:107, took 0.125686 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,499 INFO codegen.CodeGenerator: Code generated in 32.053348 ms\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,506 INFO scheduler.DAGScheduler: Registering RDD 139 (collect at AnalysisRunner.scala:326) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,507 INFO scheduler.DAGScheduler: Got map stage job 23 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,507 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,507 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,508 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,508 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,517 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 87.4 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,518 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,519 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.117.124:39417 (size: 27.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,520 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,520 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,520 INFO cluster.YarnScheduler: Adding task set 33.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,522 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 26) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,537 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:33575 (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,625 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 26) in 103 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,625 INFO cluster.YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,626 INFO scheduler.DAGScheduler: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326) finished in 0.117 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,627 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,628 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,628 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,628 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,703 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,704 INFO scheduler.DAGScheduler: Got job 24 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,704 INFO scheduler.DAGScheduler: Final stage: ResultStage 35 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,704 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,704 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,705 INFO scheduler.DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,706 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 67.7 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,708 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,708 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.117.124:39417 (size: 19.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,709 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,709 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,709 INFO cluster.YarnScheduler: Adding task set 35.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,713 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 27) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,734 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:33575 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,742 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,748 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 27) in 35 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,748 INFO cluster.YarnScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,748 INFO scheduler.DAGScheduler: ResultStage 35 (collect at AnalysisRunner.scala:326) finished in 0.043 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,749 INFO scheduler.DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,749 INFO cluster.YarnScheduler: Killing all running tasks in stage 35: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,749 INFO scheduler.DAGScheduler: Job 24 finished: collect at AnalysisRunner.scala:326, took 0.045995 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,842 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,844 INFO scheduler.DAGScheduler: Registering RDD 150 (countByKey at ColumnProfiler.scala:592) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,844 INFO scheduler.DAGScheduler: Got job 25 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,845 INFO scheduler.DAGScheduler: Final stage: ResultStage 37 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,845 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,845 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,846 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,860 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 41.8 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,862 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,863 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.117.124:39417 (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,866 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,867 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,867 INFO cluster.YarnScheduler: Adding task set 36.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,871 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 28) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,894 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-1:33575 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,955 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 28) in 84 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,956 INFO scheduler.DAGScheduler: ShuffleMapStage 36 (countByKey at ColumnProfiler.scala:592) finished in 0.109 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,956 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,956 INFO cluster.YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,956 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,957 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 37)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,957 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,957 INFO scheduler.DAGScheduler: Submitting ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,959 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 5.1 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,960 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,961 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.0.117.124:39417 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,966 INFO spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,967 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,968 INFO cluster.YarnScheduler: Adding task set 37.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,970 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 37.0 (TID 29) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,978 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-1:33575 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,981 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,988 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 37.0 (TID 29) in 18 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,989 INFO cluster.YarnScheduler: Removed TaskSet 37.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,989 INFO scheduler.DAGScheduler: ResultStage 37 (countByKey at ColumnProfiler.scala:592) finished in 0.031 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,990 INFO scheduler.DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,990 INFO cluster.YarnScheduler: Killing all running tasks in stage 37: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:08,991 INFO scheduler.DAGScheduler: Job 25 finished: countByKey at ColumnProfiler.scala:592, took 0.148325 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,137 INFO scheduler.DAGScheduler: Registering RDD 156 (collect at AnalysisRunner.scala:326) as input to shuffle 12\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,138 INFO scheduler.DAGScheduler: Got map stage job 26 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,138 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 38 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,138 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,139 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,140 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[156] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,144 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 94.7 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,146 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,146 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.0.117.124:39417 (size: 30.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,147 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,147 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[156] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,147 INFO cluster.YarnScheduler: Adding task set 38.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,149 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 30) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,162 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-1:33575 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,340 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 30) in 191 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,341 INFO cluster.YarnScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,342 INFO scheduler.DAGScheduler: ShuffleMapStage 38 (collect at AnalysisRunner.scala:326) finished in 0.202 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,344 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,344 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,344 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,344 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,381 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,382 INFO scheduler.DAGScheduler: Got job 27 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,383 INFO scheduler.DAGScheduler: Final stage: ResultStage 40 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,383 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,383 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,384 INFO scheduler.DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[159] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,395 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 179.5 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,398 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1456.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,399 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.0.117.124:39417 (size: 49.2 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,399 INFO spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,400 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[159] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,400 INFO cluster.YarnScheduler: Adding task set 40.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,402 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 31) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,415 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-1:33575 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,429 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,605 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 31) in 203 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,607 INFO cluster.YarnScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,608 INFO scheduler.DAGScheduler: ResultStage 40 (collect at AnalysisRunner.scala:326) finished in 0.221 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,608 INFO scheduler.DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,608 INFO cluster.YarnScheduler: Killing all running tasks in stage 40: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,609 INFO scheduler.DAGScheduler: Job 27 finished: collect at AnalysisRunner.scala:326, took 0.226959 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,754 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,755 INFO scheduler.DAGScheduler: Got job 28 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,755 INFO scheduler.DAGScheduler: Final stage: ResultStage 41 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,756 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,756 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,757 INFO scheduler.DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[169] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,762 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 48.9 KiB, free 1456.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,764 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1456.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,765 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.0.117.124:39417 (size: 19.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,766 INFO spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,768 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[169] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,768 INFO cluster.YarnScheduler: Adding task set 41.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,770 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 41.0 (TID 32) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,781 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on algo-1:33575 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,834 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 41.0 (TID 32) in 64 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,834 INFO cluster.YarnScheduler: Removed TaskSet 41.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,835 INFO scheduler.DAGScheduler: ResultStage 41 (treeReduce at KLLRunner.scala:107) finished in 0.078 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,835 INFO scheduler.DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,835 INFO cluster.YarnScheduler: Killing all running tasks in stage 41: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:09,835 INFO scheduler.DAGScheduler: Job 28 finished: treeReduce at KLLRunner.scala:107, took 0.081145 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,022 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on algo-1:33575 in memory (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,033 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.0.117.124:39417 in memory (size: 30.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,051 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.0.117.124:39417 in memory (size: 49.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,053 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on algo-1:33575 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,055 INFO scheduler.DAGScheduler: Registering RDD 174 (collect at AnalysisRunner.scala:326) as input to shuffle 13\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,057 INFO scheduler.DAGScheduler: Got map stage job 29 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,057 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 42 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,057 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,057 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,058 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 42 (MapPartitionsRDD[174] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,066 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 87.4 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,068 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,072 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.0.117.124:39417 (size: 27.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,072 INFO spark.SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,073 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 42 (MapPartitionsRDD[174] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,073 INFO cluster.YarnScheduler: Adding task set 42.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,074 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on 10.0.117.124:39417 in memory (size: 49.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,074 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 42.0 (TID 33) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,083 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on algo-1:33575 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,086 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on algo-1:33575 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,087 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-1:33575 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,092 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.0.117.124:39417 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,097 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 10.0.117.124:39417 in memory (size: 19.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,100 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on algo-1:33575 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,104 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 42.0 (TID 33) in 30 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,108 INFO scheduler.DAGScheduler: ShuffleMapStage 42 (collect at AnalysisRunner.scala:326) finished in 0.050 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,108 INFO cluster.YarnScheduler: Removed TaskSet 42.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,108 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,108 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,108 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,108 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,108 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.0.117.124:39417 in memory (size: 17.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,109 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on algo-1:33575 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,113 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.117.124:39417 in memory (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,114 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:33575 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,118 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.0.117.124:39417 in memory (size: 30.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,121 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-1:33575 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,128 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.0.117.124:39417 in memory (size: 27.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,130 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on algo-1:33575 in memory (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,132 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on 10.0.117.124:39417 in memory (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,134 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on algo-1:33575 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,136 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on 10.0.117.124:39417 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,138 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on algo-1:33575 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,145 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.117.124:39417 in memory (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,153 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:33575 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,155 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.0.117.124:39417 in memory (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,169 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-1:33575 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,174 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,175 INFO scheduler.DAGScheduler: Got job 30 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,175 INFO scheduler.DAGScheduler: Final stage: ResultStage 44 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,175 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 43)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,175 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,176 INFO scheduler.DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[177] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,178 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 67.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,179 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,180 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.0.117.124:39417 (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,180 INFO spark.SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,181 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[177] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,181 INFO cluster.YarnScheduler: Adding task set 44.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,182 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 44.0 (TID 34) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,191 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on algo-1:33575 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,196 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,204 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 44.0 (TID 34) in 22 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,204 INFO cluster.YarnScheduler: Removed TaskSet 44.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,205 INFO scheduler.DAGScheduler: ResultStage 44 (collect at AnalysisRunner.scala:326) finished in 0.028 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,205 INFO scheduler.DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,206 INFO cluster.YarnScheduler: Killing all running tasks in stage 44: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,206 INFO scheduler.DAGScheduler: Job 30 finished: collect at AnalysisRunner.scala:326, took 0.031714 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,254 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,255 INFO scheduler.DAGScheduler: Registering RDD 185 (countByKey at ColumnProfiler.scala:592) as input to shuffle 14\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,255 INFO scheduler.DAGScheduler: Got job 31 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,256 INFO scheduler.DAGScheduler: Final stage: ResultStage 46 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,256 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 45)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,256 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 45)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,257 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 45 (MapPartitionsRDD[185] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,263 INFO memory.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 41.8 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,264 INFO memory.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,265 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.0.117.124:39417 (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,266 INFO spark.SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,266 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 45 (MapPartitionsRDD[185] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,266 INFO cluster.YarnScheduler: Adding task set 45.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,267 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 45.0 (TID 35) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,276 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on algo-1:33575 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,325 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 45.0 (TID 35) in 57 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,325 INFO cluster.YarnScheduler: Removed TaskSet 45.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,326 INFO scheduler.DAGScheduler: ShuffleMapStage 45 (countByKey at ColumnProfiler.scala:592) finished in 0.069 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,326 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,327 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,327 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 46)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,327 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,327 INFO scheduler.DAGScheduler: Submitting ResultStage 46 (ShuffledRDD[186] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,329 INFO memory.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 5.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,331 INFO memory.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,331 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.0.117.124:39417 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,331 INFO spark.SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,332 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (ShuffledRDD[186] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,332 INFO cluster.YarnScheduler: Adding task set 46.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,333 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 46.0 (TID 36) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,342 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on algo-1:33575 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,345 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,354 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 46.0 (TID 36) in 21 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,354 INFO cluster.YarnScheduler: Removed TaskSet 46.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,355 INFO scheduler.DAGScheduler: ResultStage 46 (countByKey at ColumnProfiler.scala:592) finished in 0.027 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,356 INFO scheduler.DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,356 INFO cluster.YarnScheduler: Killing all running tasks in stage 46: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,357 INFO scheduler.DAGScheduler: Job 31 finished: countByKey at ColumnProfiler.scala:592, took 0.102559 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,494 INFO scheduler.DAGScheduler: Registering RDD 191 (collect at AnalysisRunner.scala:326) as input to shuffle 15\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,494 INFO scheduler.DAGScheduler: Got map stage job 32 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,494 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 47 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,494 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,495 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,496 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 47 (MapPartitionsRDD[191] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,499 INFO memory.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 94.7 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,501 INFO memory.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,502 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.0.117.124:39417 (size: 30.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,502 INFO spark.SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,503 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 47 (MapPartitionsRDD[191] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,503 INFO cluster.YarnScheduler: Adding task set 47.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,504 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 47.0 (TID 37) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,515 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on algo-1:33575 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,642 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 47.0 (TID 37) in 138 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,642 INFO cluster.YarnScheduler: Removed TaskSet 47.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,643 INFO scheduler.DAGScheduler: ShuffleMapStage 47 (collect at AnalysisRunner.scala:326) finished in 0.147 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,643 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,644 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,644 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,644 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,679 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,680 INFO scheduler.DAGScheduler: Got job 33 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,680 INFO scheduler.DAGScheduler: Final stage: ResultStage 49 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,681 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 48)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,681 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,681 INFO scheduler.DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[194] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,689 INFO memory.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 179.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,691 INFO memory.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 49.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,691 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.0.117.124:39417 (size: 49.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,692 INFO spark.SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,692 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[194] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,693 INFO cluster.YarnScheduler: Adding task set 49.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,695 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 49.0 (TID 38) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,702 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on algo-1:33575 (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,709 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,799 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 49.0 (TID 38) in 105 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,799 INFO cluster.YarnScheduler: Removed TaskSet 49.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,799 INFO scheduler.DAGScheduler: ResultStage 49 (collect at AnalysisRunner.scala:326) finished in 0.116 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,800 INFO scheduler.DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,800 INFO cluster.YarnScheduler: Killing all running tasks in stage 49: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,800 INFO scheduler.DAGScheduler: Job 33 finished: collect at AnalysisRunner.scala:326, took 0.120366 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,890 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,894 INFO scheduler.DAGScheduler: Got job 34 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,894 INFO scheduler.DAGScheduler: Final stage: ResultStage 50 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,894 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,895 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,895 INFO scheduler.DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[204] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,901 INFO memory.MemoryStore: Block broadcast_41 stored as values in memory (estimated size 48.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,902 INFO memory.MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,903 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.0.117.124:39417 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,903 INFO spark.SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,908 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[204] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,908 INFO cluster.YarnScheduler: Adding task set 50.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,910 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 50.0 (TID 39) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,919 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on algo-1:33575 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,962 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 50.0 (TID 39) in 52 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,963 INFO cluster.YarnScheduler: Removed TaskSet 50.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,963 INFO scheduler.DAGScheduler: ResultStage 50 (treeReduce at KLLRunner.scala:107) finished in 0.067 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,964 INFO scheduler.DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,964 INFO cluster.YarnScheduler: Killing all running tasks in stage 50: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:10,964 INFO scheduler.DAGScheduler: Job 34 finished: treeReduce at KLLRunner.scala:107, took 0.073831 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,068 INFO scheduler.DAGScheduler: Registering RDD 209 (collect at AnalysisRunner.scala:326) as input to shuffle 16\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,069 INFO scheduler.DAGScheduler: Got map stage job 35 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,069 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 51 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,069 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,070 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,070 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 51 (MapPartitionsRDD[209] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,073 INFO memory.MemoryStore: Block broadcast_42 stored as values in memory (estimated size 87.4 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,074 INFO memory.MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,075 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.0.117.124:39417 (size: 27.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,075 INFO spark.SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,076 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 51 (MapPartitionsRDD[209] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,076 INFO cluster.YarnScheduler: Adding task set 51.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,077 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 51.0 (TID 40) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,089 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on algo-1:33575 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,107 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 51.0 (TID 40) in 30 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,107 INFO cluster.YarnScheduler: Removed TaskSet 51.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,108 INFO scheduler.DAGScheduler: ShuffleMapStage 51 (collect at AnalysisRunner.scala:326) finished in 0.037 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,109 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,109 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,110 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,110 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,161 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,163 INFO scheduler.DAGScheduler: Got job 36 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,163 INFO scheduler.DAGScheduler: Final stage: ResultStage 53 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,164 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 52)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,164 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,164 INFO scheduler.DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[212] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,167 INFO memory.MemoryStore: Block broadcast_43 stored as values in memory (estimated size 67.7 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,169 INFO memory.MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,170 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.0.117.124:39417 (size: 19.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,174 INFO spark.SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,174 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[212] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,174 INFO cluster.YarnScheduler: Adding task set 53.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,177 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 53.0 (TID 41) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,185 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on algo-1:33575 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,188 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,194 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 53.0 (TID 41) in 17 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,194 INFO cluster.YarnScheduler: Removed TaskSet 53.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,194 INFO scheduler.DAGScheduler: ResultStage 53 (collect at AnalysisRunner.scala:326) finished in 0.029 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,195 INFO scheduler.DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,195 INFO cluster.YarnScheduler: Killing all running tasks in stage 53: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,195 INFO scheduler.DAGScheduler: Job 36 finished: collect at AnalysisRunner.scala:326, took 0.033020 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,256 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,257 INFO scheduler.DAGScheduler: Registering RDD 220 (countByKey at ColumnProfiler.scala:592) as input to shuffle 17\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,258 INFO scheduler.DAGScheduler: Got job 37 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,258 INFO scheduler.DAGScheduler: Final stage: ResultStage 55 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,258 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 54)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,258 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 54)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,259 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 54 (MapPartitionsRDD[220] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,269 INFO memory.MemoryStore: Block broadcast_44 stored as values in memory (estimated size 41.8 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,271 INFO memory.MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,272 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.0.117.124:39417 (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,273 INFO spark.SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,273 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 54 (MapPartitionsRDD[220] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,273 INFO cluster.YarnScheduler: Adding task set 54.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,275 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 54.0 (TID 42) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,283 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on algo-1:33575 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,322 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 54.0 (TID 42) in 47 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,322 INFO cluster.YarnScheduler: Removed TaskSet 54.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,323 INFO scheduler.DAGScheduler: ShuffleMapStage 54 (countByKey at ColumnProfiler.scala:592) finished in 0.062 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,323 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,323 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,324 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 55)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,324 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,324 INFO scheduler.DAGScheduler: Submitting ResultStage 55 (ShuffledRDD[221] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,326 INFO memory.MemoryStore: Block broadcast_45 stored as values in memory (estimated size 5.1 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,327 INFO memory.MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,327 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.0.117.124:39417 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,328 INFO spark.SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,328 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 55 (ShuffledRDD[221] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,329 INFO cluster.YarnScheduler: Adding task set 55.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,330 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 55.0 (TID 43) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,362 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on algo-1:33575 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,367 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 17 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,379 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 55.0 (TID 43) in 49 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,379 INFO cluster.YarnScheduler: Removed TaskSet 55.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,379 INFO scheduler.DAGScheduler: ResultStage 55 (countByKey at ColumnProfiler.scala:592) finished in 0.054 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,380 INFO scheduler.DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,380 INFO cluster.YarnScheduler: Killing all running tasks in stage 55: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,380 INFO scheduler.DAGScheduler: Job 37 finished: countByKey at ColumnProfiler.scala:592, took 0.123796 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,491 INFO scheduler.DAGScheduler: Registering RDD 226 (collect at AnalysisRunner.scala:326) as input to shuffle 18\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,491 INFO scheduler.DAGScheduler: Got map stage job 38 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,491 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 56 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,491 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,492 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,492 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 56 (MapPartitionsRDD[226] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,496 INFO memory.MemoryStore: Block broadcast_46 stored as values in memory (estimated size 94.7 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,497 INFO memory.MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,497 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on 10.0.117.124:39417 (size: 30.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,498 INFO spark.SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,498 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 56 (MapPartitionsRDD[226] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,498 INFO cluster.YarnScheduler: Adding task set 56.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,499 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 56.0 (TID 44) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,508 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on algo-1:33575 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,605 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 56.0 (TID 44) in 106 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,605 INFO cluster.YarnScheduler: Removed TaskSet 56.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,606 INFO scheduler.DAGScheduler: ShuffleMapStage 56 (collect at AnalysisRunner.scala:326) finished in 0.114 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,606 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,607 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,607 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,607 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,647 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,648 INFO scheduler.DAGScheduler: Got job 39 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,648 INFO scheduler.DAGScheduler: Final stage: ResultStage 58 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,648 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 57)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,649 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,649 INFO scheduler.DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[229] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,657 INFO memory.MemoryStore: Block broadcast_47 stored as values in memory (estimated size 179.5 KiB, free 1456.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,659 INFO memory.MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 49.1 KiB, free 1456.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,659 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on 10.0.117.124:39417 (size: 49.1 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,660 INFO spark.SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,660 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 58 (MapPartitionsRDD[229] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,661 INFO cluster.YarnScheduler: Adding task set 58.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,662 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 58.0 (TID 45) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,675 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on algo-1:33575 (size: 49.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,685 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,768 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 58.0 (TID 45) in 106 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,768 INFO cluster.YarnScheduler: Removed TaskSet 58.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,769 INFO scheduler.DAGScheduler: ResultStage 58 (collect at AnalysisRunner.scala:326) finished in 0.119 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,769 INFO scheduler.DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,769 INFO cluster.YarnScheduler: Killing all running tasks in stage 58: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,769 INFO scheduler.DAGScheduler: Job 39 finished: collect at AnalysisRunner.scala:326, took 0.122064 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,974 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,975 INFO scheduler.DAGScheduler: Got job 40 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,975 INFO scheduler.DAGScheduler: Final stage: ResultStage 59 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,975 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,977 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,978 INFO scheduler.DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[239] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,987 INFO memory.MemoryStore: Block broadcast_48 stored as values in memory (estimated size 48.9 KiB, free 1456.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,989 INFO memory.MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1456.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,990 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on 10.0.117.124:39417 (size: 19.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,990 INFO spark.SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,991 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 59 (MapPartitionsRDD[239] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,991 INFO cluster.YarnScheduler: Adding task set 59.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:11,992 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 59.0 (TID 46) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,002 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on algo-1:33575 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,074 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 59.0 (TID 46) in 82 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,074 INFO cluster.YarnScheduler: Removed TaskSet 59.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,074 INFO scheduler.DAGScheduler: ResultStage 59 (treeReduce at KLLRunner.scala:107) finished in 0.095 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,075 INFO scheduler.DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,075 INFO cluster.YarnScheduler: Killing all running tasks in stage 59: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,075 INFO scheduler.DAGScheduler: Job 40 finished: treeReduce at KLLRunner.scala:107, took 0.100770 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,226 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on algo-1:33575 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,256 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on 10.0.117.124:39417 in memory (size: 30.1 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,261 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on algo-1:33575 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,266 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on 10.0.117.124:39417 in memory (size: 27.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,282 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on algo-1:33575 in memory (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,282 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on 10.0.117.124:39417 in memory (size: 49.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,289 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on 10.0.117.124:39417 in memory (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,292 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on algo-1:33575 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,296 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on 10.0.117.124:39417 in memory (size: 49.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,301 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on algo-1:33575 in memory (size: 49.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,309 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on algo-1:33575 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,318 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on 10.0.117.124:39417 in memory (size: 27.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,322 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on algo-1:33575 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,324 INFO scheduler.DAGScheduler: Registering RDD 244 (collect at AnalysisRunner.scala:326) as input to shuffle 19\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,324 INFO scheduler.DAGScheduler: Got map stage job 41 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,324 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 60 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,328 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,329 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on 10.0.117.124:39417 in memory (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,330 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,330 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 60 (MapPartitionsRDD[244] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,333 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on 10.0.117.124:39417 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,337 INFO memory.MemoryStore: Block broadcast_49 stored as values in memory (estimated size 87.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,339 INFO memory.MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,339 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on 10.0.117.124:39417 (size: 27.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,339 INFO spark.SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,340 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 60 (MapPartitionsRDD[244] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,340 INFO cluster.YarnScheduler: Adding task set 60.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,341 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 60.0 (TID 47) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,349 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on algo-1:33575 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,356 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on 10.0.117.124:39417 in memory (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,356 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on algo-1:33575 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,357 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on algo-1:33575 (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,365 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on 10.0.117.124:39417 in memory (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,365 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on algo-1:33575 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,369 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on 10.0.117.124:39417 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,377 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on algo-1:33575 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,381 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 60.0 (TID 47) in 40 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,381 INFO cluster.YarnScheduler: Removed TaskSet 60.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,383 INFO scheduler.DAGScheduler: ShuffleMapStage 60 (collect at AnalysisRunner.scala:326) finished in 0.049 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,384 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,384 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,384 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,385 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,384 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on 10.0.117.124:39417 in memory (size: 30.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,387 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on algo-1:33575 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,407 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on 10.0.117.124:39417 in memory (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,408 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on algo-1:33575 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,411 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on 10.0.117.124:39417 in memory (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,412 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on algo-1:33575 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,460 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,461 INFO scheduler.DAGScheduler: Got job 42 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,461 INFO scheduler.DAGScheduler: Final stage: ResultStage 62 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,461 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 61)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,461 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,462 INFO scheduler.DAGScheduler: Submitting ResultStage 62 (MapPartitionsRDD[247] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,463 INFO memory.MemoryStore: Block broadcast_50 stored as values in memory (estimated size 67.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,465 INFO memory.MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,466 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on 10.0.117.124:39417 (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,466 INFO spark.SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,466 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 62 (MapPartitionsRDD[247] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,467 INFO cluster.YarnScheduler: Adding task set 62.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,468 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 62.0 (TID 48) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,476 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on algo-1:33575 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,485 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 19 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,490 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 62.0 (TID 48) in 22 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,490 INFO cluster.YarnScheduler: Removed TaskSet 62.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,491 INFO scheduler.DAGScheduler: ResultStage 62 (collect at AnalysisRunner.scala:326) finished in 0.028 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,491 INFO scheduler.DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,491 INFO cluster.YarnScheduler: Killing all running tasks in stage 62: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,491 INFO scheduler.DAGScheduler: Job 42 finished: collect at AnalysisRunner.scala:326, took 0.031089 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,533 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,534 INFO scheduler.DAGScheduler: Registering RDD 255 (countByKey at ColumnProfiler.scala:592) as input to shuffle 20\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,534 INFO scheduler.DAGScheduler: Got job 43 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,534 INFO scheduler.DAGScheduler: Final stage: ResultStage 64 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,534 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 63)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,534 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 63)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,535 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 63 (MapPartitionsRDD[255] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,541 INFO memory.MemoryStore: Block broadcast_51 stored as values in memory (estimated size 41.8 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,542 INFO memory.MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,543 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on 10.0.117.124:39417 (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,543 INFO spark.SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,544 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 63 (MapPartitionsRDD[255] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,544 INFO cluster.YarnScheduler: Adding task set 63.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,546 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 63.0 (TID 49) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,559 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on algo-1:33575 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,606 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 63.0 (TID 49) in 60 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,607 INFO cluster.YarnScheduler: Removed TaskSet 63.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,607 INFO scheduler.DAGScheduler: ShuffleMapStage 63 (countByKey at ColumnProfiler.scala:592) finished in 0.071 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,608 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,609 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,609 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 64)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,609 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,609 INFO scheduler.DAGScheduler: Submitting ResultStage 64 (ShuffledRDD[256] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,614 INFO memory.MemoryStore: Block broadcast_52 stored as values in memory (estimated size 5.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,617 INFO memory.MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,617 INFO storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on 10.0.117.124:39417 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,618 INFO spark.SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,618 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 64 (ShuffledRDD[256] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,621 INFO cluster.YarnScheduler: Adding task set 64.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,623 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 64.0 (TID 50) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,632 INFO storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on algo-1:33575 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,636 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 20 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,647 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 64.0 (TID 50) in 24 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,647 INFO cluster.YarnScheduler: Removed TaskSet 64.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,648 INFO scheduler.DAGScheduler: ResultStage 64 (countByKey at ColumnProfiler.scala:592) finished in 0.038 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,648 INFO scheduler.DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,649 INFO cluster.YarnScheduler: Killing all running tasks in stage 64: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,650 INFO scheduler.DAGScheduler: Job 43 finished: countByKey at ColumnProfiler.scala:592, took 0.116652 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,915 INFO scheduler.DAGScheduler: Registering RDD 261 (collect at AnalysisRunner.scala:326) as input to shuffle 21\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,916 INFO scheduler.DAGScheduler: Got map stage job 44 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,916 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 65 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,916 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,919 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,919 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 65 (MapPartitionsRDD[261] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,927 INFO memory.MemoryStore: Block broadcast_53 stored as values in memory (estimated size 94.7 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,929 INFO memory.MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,930 INFO storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on 10.0.117.124:39417 (size: 30.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,930 INFO spark.SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,931 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 65 (MapPartitionsRDD[261] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,931 INFO cluster.YarnScheduler: Adding task set 65.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,932 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 65.0 (TID 51) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:12,943 INFO storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on algo-1:33575 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,054 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 65.0 (TID 51) in 122 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,054 INFO cluster.YarnScheduler: Removed TaskSet 65.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,055 INFO scheduler.DAGScheduler: ShuffleMapStage 65 (collect at AnalysisRunner.scala:326) finished in 0.132 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,055 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,055 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,055 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,055 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,090 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,091 INFO scheduler.DAGScheduler: Got job 45 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,092 INFO scheduler.DAGScheduler: Final stage: ResultStage 67 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,095 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 66)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,095 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,095 INFO scheduler.DAGScheduler: Submitting ResultStage 67 (MapPartitionsRDD[264] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,108 INFO memory.MemoryStore: Block broadcast_54 stored as values in memory (estimated size 179.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,110 INFO memory.MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,110 INFO storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on 10.0.117.124:39417 (size: 49.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,111 INFO spark.SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,112 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 67 (MapPartitionsRDD[264] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,112 INFO cluster.YarnScheduler: Adding task set 67.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,113 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 67.0 (TID 52) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,121 INFO storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on algo-1:33575 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,138 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 21 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,230 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 67.0 (TID 52) in 117 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,230 INFO cluster.YarnScheduler: Removed TaskSet 67.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,231 INFO scheduler.DAGScheduler: ResultStage 67 (collect at AnalysisRunner.scala:326) finished in 0.130 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,231 INFO scheduler.DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,232 INFO cluster.YarnScheduler: Killing all running tasks in stage 67: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,232 INFO scheduler.DAGScheduler: Job 45 finished: collect at AnalysisRunner.scala:326, took 0.141278 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,403 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,405 INFO scheduler.DAGScheduler: Got job 46 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,405 INFO scheduler.DAGScheduler: Final stage: ResultStage 68 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,405 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,406 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,406 INFO scheduler.DAGScheduler: Submitting ResultStage 68 (MapPartitionsRDD[274] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,417 INFO memory.MemoryStore: Block broadcast_55 stored as values in memory (estimated size 48.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,419 INFO memory.MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,421 INFO storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on 10.0.117.124:39417 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,421 INFO spark.SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,422 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 68 (MapPartitionsRDD[274] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,422 INFO cluster.YarnScheduler: Adding task set 68.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,426 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 68.0 (TID 53) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,433 INFO storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on algo-1:33575 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,473 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 68.0 (TID 53) in 48 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,475 INFO cluster.YarnScheduler: Removed TaskSet 68.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,476 INFO scheduler.DAGScheduler: ResultStage 68 (treeReduce at KLLRunner.scala:107) finished in 0.064 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,477 INFO scheduler.DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,478 INFO cluster.YarnScheduler: Killing all running tasks in stage 68: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,478 INFO scheduler.DAGScheduler: Job 46 finished: treeReduce at KLLRunner.scala:107, took 0.074012 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,596 INFO scheduler.DAGScheduler: Registering RDD 279 (collect at AnalysisRunner.scala:326) as input to shuffle 22\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,596 INFO scheduler.DAGScheduler: Got map stage job 47 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,596 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 69 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,596 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,597 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,597 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 69 (MapPartitionsRDD[279] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,601 INFO memory.MemoryStore: Block broadcast_56 stored as values in memory (estimated size 87.4 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,602 INFO memory.MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,603 INFO storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on 10.0.117.124:39417 (size: 27.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,603 INFO spark.SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,603 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 69 (MapPartitionsRDD[279] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,603 INFO cluster.YarnScheduler: Adding task set 69.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,604 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 69.0 (TID 54) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,613 INFO storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on algo-1:33575 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,627 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 69.0 (TID 54) in 23 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,627 INFO cluster.YarnScheduler: Removed TaskSet 69.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,632 INFO scheduler.DAGScheduler: ShuffleMapStage 69 (collect at AnalysisRunner.scala:326) finished in 0.035 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,633 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,633 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,633 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,633 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,696 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,697 INFO scheduler.DAGScheduler: Got job 48 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,698 INFO scheduler.DAGScheduler: Final stage: ResultStage 71 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,698 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 70)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,698 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,699 INFO scheduler.DAGScheduler: Submitting ResultStage 71 (MapPartitionsRDD[282] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,700 INFO memory.MemoryStore: Block broadcast_57 stored as values in memory (estimated size 67.7 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,702 INFO memory.MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,702 INFO storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on 10.0.117.124:39417 (size: 19.8 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,703 INFO spark.SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,703 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 71 (MapPartitionsRDD[282] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,704 INFO cluster.YarnScheduler: Adding task set 71.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,706 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 71.0 (TID 55) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,716 INFO storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on algo-1:33575 (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,725 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 22 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,733 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 71.0 (TID 55) in 27 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,733 INFO cluster.YarnScheduler: Removed TaskSet 71.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,734 INFO scheduler.DAGScheduler: ResultStage 71 (collect at AnalysisRunner.scala:326) finished in 0.035 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,734 INFO scheduler.DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,734 INFO cluster.YarnScheduler: Killing all running tasks in stage 71: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,734 INFO scheduler.DAGScheduler: Job 48 finished: collect at AnalysisRunner.scala:326, took 0.037245 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,785 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,786 INFO scheduler.DAGScheduler: Registering RDD 290 (countByKey at ColumnProfiler.scala:592) as input to shuffle 23\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,787 INFO scheduler.DAGScheduler: Got job 49 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,787 INFO scheduler.DAGScheduler: Final stage: ResultStage 73 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,787 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 72)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,787 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 72)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,788 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 72 (MapPartitionsRDD[290] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,800 INFO memory.MemoryStore: Block broadcast_58 stored as values in memory (estimated size 41.8 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,802 INFO memory.MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,802 INFO storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on 10.0.117.124:39417 (size: 17.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,803 INFO spark.SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,803 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 72 (MapPartitionsRDD[290] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,804 INFO cluster.YarnScheduler: Adding task set 72.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,805 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 72.0 (TID 56) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,813 INFO storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on algo-1:33575 (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,836 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 72.0 (TID 56) in 31 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,836 INFO cluster.YarnScheduler: Removed TaskSet 72.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,836 INFO scheduler.DAGScheduler: ShuffleMapStage 72 (countByKey at ColumnProfiler.scala:592) finished in 0.045 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,836 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,836 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,836 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 73)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,836 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,837 INFO scheduler.DAGScheduler: Submitting ResultStage 73 (ShuffledRDD[291] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,838 INFO memory.MemoryStore: Block broadcast_59 stored as values in memory (estimated size 5.1 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,839 INFO memory.MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,839 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on 10.0.117.124:39417 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,840 INFO spark.SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,840 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 73 (ShuffledRDD[291] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,840 INFO cluster.YarnScheduler: Adding task set 73.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,841 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 73.0 (TID 57) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,851 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on algo-1:33575 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,853 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 23 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,860 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 73.0 (TID 57) in 19 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,860 INFO cluster.YarnScheduler: Removed TaskSet 73.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,861 INFO scheduler.DAGScheduler: ResultStage 73 (countByKey at ColumnProfiler.scala:592) finished in 0.024 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,862 INFO scheduler.DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,862 INFO cluster.YarnScheduler: Killing all running tasks in stage 73: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,862 INFO scheduler.DAGScheduler: Job 49 finished: countByKey at ColumnProfiler.scala:592, took 0.076157 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,942 INFO scheduler.DAGScheduler: Registering RDD 296 (collect at AnalysisRunner.scala:326) as input to shuffle 24\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,942 INFO scheduler.DAGScheduler: Got map stage job 50 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,942 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 74 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,942 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,943 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,943 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 74 (MapPartitionsRDD[296] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,947 INFO memory.MemoryStore: Block broadcast_60 stored as values in memory (estimated size 94.7 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,948 INFO memory.MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,948 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on 10.0.117.124:39417 (size: 30.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,949 INFO spark.SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,949 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 74 (MapPartitionsRDD[296] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,949 INFO cluster.YarnScheduler: Adding task set 74.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,950 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 74.0 (TID 58) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:13,961 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on algo-1:33575 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,054 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 74.0 (TID 58) in 104 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,054 INFO cluster.YarnScheduler: Removed TaskSet 74.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,055 INFO scheduler.DAGScheduler: ShuffleMapStage 74 (collect at AnalysisRunner.scala:326) finished in 0.111 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,060 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,060 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,060 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,060 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,097 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,097 INFO scheduler.DAGScheduler: Got job 51 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,097 INFO scheduler.DAGScheduler: Final stage: ResultStage 76 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,097 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 75)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,098 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,098 INFO scheduler.DAGScheduler: Submitting ResultStage 76 (MapPartitionsRDD[299] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,102 INFO memory.MemoryStore: Block broadcast_61 stored as values in memory (estimated size 179.5 KiB, free 1456.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,104 INFO memory.MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 49.1 KiB, free 1456.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,104 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on 10.0.117.124:39417 (size: 49.1 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,105 INFO spark.SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,105 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 76 (MapPartitionsRDD[299] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,105 INFO cluster.YarnScheduler: Adding task set 76.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,106 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 76.0 (TID 59) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,115 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on algo-1:33575 (size: 49.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,130 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 24 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,234 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 76.0 (TID 59) in 128 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,234 INFO cluster.YarnScheduler: Removed TaskSet 76.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,235 INFO scheduler.DAGScheduler: ResultStage 76 (collect at AnalysisRunner.scala:326) finished in 0.137 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,235 INFO scheduler.DAGScheduler: Job 51 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,235 INFO cluster.YarnScheduler: Killing all running tasks in stage 76: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,235 INFO scheduler.DAGScheduler: Job 51 finished: collect at AnalysisRunner.scala:326, took 0.138293 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,314 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,314 INFO scheduler.DAGScheduler: Got job 52 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,315 INFO scheduler.DAGScheduler: Final stage: ResultStage 77 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,315 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,315 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,315 INFO scheduler.DAGScheduler: Submitting ResultStage 77 (MapPartitionsRDD[309] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,319 INFO memory.MemoryStore: Block broadcast_62 stored as values in memory (estimated size 48.9 KiB, free 1456.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,320 INFO memory.MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1456.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,321 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on 10.0.117.124:39417 (size: 19.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,321 INFO spark.SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,321 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 77 (MapPartitionsRDD[309] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,321 INFO cluster.YarnScheduler: Adding task set 77.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,322 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 77.0 (TID 60) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,328 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on algo-1:33575 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,387 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 77.0 (TID 60) in 65 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,387 INFO cluster.YarnScheduler: Removed TaskSet 77.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,388 INFO scheduler.DAGScheduler: ResultStage 77 (treeReduce at KLLRunner.scala:107) finished in 0.072 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,388 INFO scheduler.DAGScheduler: Job 52 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,388 INFO cluster.YarnScheduler: Killing all running tasks in stage 77: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,389 INFO scheduler.DAGScheduler: Job 52 finished: treeReduce at KLLRunner.scala:107, took 0.074671 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,493 INFO storage.BlockManagerInfo: Removed broadcast_56_piece0 on 10.0.117.124:39417 in memory (size: 27.3 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,495 INFO storage.BlockManagerInfo: Removed broadcast_56_piece0 on algo-1:33575 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,497 INFO storage.BlockManagerInfo: Removed broadcast_55_piece0 on 10.0.117.124:39417 in memory (size: 19.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,498 INFO storage.BlockManagerInfo: Removed broadcast_55_piece0 on algo-1:33575 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,500 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on 10.0.117.124:39417 in memory (size: 17.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,502 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on algo-1:33575 in memory (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,504 INFO storage.BlockManagerInfo: Removed broadcast_60_piece0 on 10.0.117.124:39417 in memory (size: 30.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,506 INFO storage.BlockManagerInfo: Removed broadcast_60_piece0 on algo-1:33575 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,509 INFO storage.BlockManagerInfo: Removed broadcast_62_piece0 on 10.0.117.124:39417 in memory (size: 19.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,511 INFO storage.BlockManagerInfo: Removed broadcast_62_piece0 on algo-1:33575 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,513 INFO storage.BlockManagerInfo: Removed broadcast_52_piece0 on 10.0.117.124:39417 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,515 INFO storage.BlockManagerInfo: Removed broadcast_52_piece0 on algo-1:33575 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,516 INFO storage.BlockManagerInfo: Removed broadcast_54_piece0 on 10.0.117.124:39417 in memory (size: 49.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,518 INFO storage.BlockManagerInfo: Removed broadcast_54_piece0 on algo-1:33575 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,519 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on 10.0.117.124:39417 in memory (size: 27.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,521 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on algo-1:33575 in memory (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,522 INFO storage.BlockManagerInfo: Removed broadcast_53_piece0 on 10.0.117.124:39417 in memory (size: 30.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,523 INFO storage.BlockManagerInfo: Removed broadcast_53_piece0 on algo-1:33575 in memory (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,524 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on 10.0.117.124:39417 in memory (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,525 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on algo-1:33575 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,527 INFO storage.BlockManagerInfo: Removed broadcast_57_piece0 on 10.0.117.124:39417 in memory (size: 19.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,527 INFO storage.BlockManagerInfo: Removed broadcast_57_piece0 on algo-1:33575 in memory (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,530 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on 10.0.117.124:39417 in memory (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,531 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on algo-1:33575 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,533 INFO storage.BlockManagerInfo: Removed broadcast_61_piece0 on 10.0.117.124:39417 in memory (size: 49.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,534 INFO storage.BlockManagerInfo: Removed broadcast_61_piece0 on algo-1:33575 in memory (size: 49.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,537 INFO storage.BlockManagerInfo: Removed broadcast_59_piece0 on 10.0.117.124:39417 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,538 INFO storage.BlockManagerInfo: Removed broadcast_59_piece0 on algo-1:33575 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,569 INFO scheduler.DAGScheduler: Registering RDD 314 (collect at AnalysisRunner.scala:326) as input to shuffle 25\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,569 INFO scheduler.DAGScheduler: Got map stage job 53 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,569 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 78 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,569 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,569 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,570 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 78 (MapPartitionsRDD[314] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,572 INFO memory.MemoryStore: Block broadcast_63 stored as values in memory (estimated size 87.4 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,573 INFO memory.MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,573 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on 10.0.117.124:39417 (size: 27.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,574 INFO spark.SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,574 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 78 (MapPartitionsRDD[314] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,574 INFO cluster.YarnScheduler: Adding task set 78.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,575 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 78.0 (TID 61) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,583 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on algo-1:33575 (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,592 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 78.0 (TID 61) in 17 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,592 INFO cluster.YarnScheduler: Removed TaskSet 78.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,593 INFO scheduler.DAGScheduler: ShuffleMapStage 78 (collect at AnalysisRunner.scala:326) finished in 0.023 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,593 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,593 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,594 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,594 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,637 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,638 INFO scheduler.DAGScheduler: Got job 54 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,638 INFO scheduler.DAGScheduler: Final stage: ResultStage 80 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,638 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 79)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,638 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,639 INFO scheduler.DAGScheduler: Submitting ResultStage 80 (MapPartitionsRDD[317] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,646 INFO memory.MemoryStore: Block broadcast_64 stored as values in memory (estimated size 67.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,647 INFO memory.MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,648 INFO storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on 10.0.117.124:39417 (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,648 INFO spark.SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,648 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 80 (MapPartitionsRDD[317] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,649 INFO cluster.YarnScheduler: Adding task set 80.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,650 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 80.0 (TID 62) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,658 INFO storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on algo-1:33575 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,661 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 25 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,666 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 80.0 (TID 62) in 16 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,667 INFO cluster.YarnScheduler: Removed TaskSet 80.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,667 INFO scheduler.DAGScheduler: ResultStage 80 (collect at AnalysisRunner.scala:326) finished in 0.022 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,668 INFO scheduler.DAGScheduler: Job 54 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,668 INFO cluster.YarnScheduler: Killing all running tasks in stage 80: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,668 INFO scheduler.DAGScheduler: Job 54 finished: collect at AnalysisRunner.scala:326, took 0.030968 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,713 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,714 INFO scheduler.DAGScheduler: Registering RDD 325 (countByKey at ColumnProfiler.scala:592) as input to shuffle 26\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,715 INFO scheduler.DAGScheduler: Got job 55 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,715 INFO scheduler.DAGScheduler: Final stage: ResultStage 82 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,715 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 81)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,715 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 81)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,716 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 81 (MapPartitionsRDD[325] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,720 INFO memory.MemoryStore: Block broadcast_65 stored as values in memory (estimated size 41.8 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,721 INFO memory.MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,721 INFO storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on 10.0.117.124:39417 (size: 17.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,722 INFO spark.SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,722 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 81 (MapPartitionsRDD[325] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,722 INFO cluster.YarnScheduler: Adding task set 81.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,728 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 81.0 (TID 63) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,735 INFO storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on algo-1:33575 (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,785 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 81.0 (TID 63) in 56 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,785 INFO cluster.YarnScheduler: Removed TaskSet 81.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,785 INFO scheduler.DAGScheduler: ShuffleMapStage 81 (countByKey at ColumnProfiler.scala:592) finished in 0.068 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,786 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,786 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,786 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 82)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,786 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,787 INFO scheduler.DAGScheduler: Submitting ResultStage 82 (ShuffledRDD[326] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,788 INFO memory.MemoryStore: Block broadcast_66 stored as values in memory (estimated size 5.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,789 INFO memory.MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,790 INFO storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on 10.0.117.124:39417 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,790 INFO spark.SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,791 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 82 (ShuffledRDD[326] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,791 INFO cluster.YarnScheduler: Adding task set 82.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,792 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 82.0 (TID 64) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,798 INFO storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on algo-1:33575 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,802 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 26 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,810 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 82.0 (TID 64) in 18 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,810 INFO cluster.YarnScheduler: Removed TaskSet 82.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,811 INFO scheduler.DAGScheduler: ResultStage 82 (countByKey at ColumnProfiler.scala:592) finished in 0.024 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,811 INFO scheduler.DAGScheduler: Job 55 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,811 INFO cluster.YarnScheduler: Killing all running tasks in stage 82: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,811 INFO scheduler.DAGScheduler: Job 55 finished: countByKey at ColumnProfiler.scala:592, took 0.097661 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,955 INFO scheduler.DAGScheduler: Registering RDD 331 (collect at AnalysisRunner.scala:326) as input to shuffle 27\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,955 INFO scheduler.DAGScheduler: Got map stage job 56 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,955 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 83 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,955 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,955 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,956 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 83 (MapPartitionsRDD[331] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,960 INFO memory.MemoryStore: Block broadcast_67 stored as values in memory (estimated size 94.7 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,961 INFO memory.MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,966 INFO storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on 10.0.117.124:39417 (size: 30.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,966 INFO spark.SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,967 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 83 (MapPartitionsRDD[331] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,967 INFO cluster.YarnScheduler: Adding task set 83.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,968 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 83.0 (TID 65) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:14,977 INFO storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on algo-1:33575 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,103 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 83.0 (TID 65) in 135 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,103 INFO cluster.YarnScheduler: Removed TaskSet 83.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,104 INFO scheduler.DAGScheduler: ShuffleMapStage 83 (collect at AnalysisRunner.scala:326) finished in 0.147 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,104 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,104 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,104 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,104 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,135 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,136 INFO scheduler.DAGScheduler: Got job 57 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,136 INFO scheduler.DAGScheduler: Final stage: ResultStage 85 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,136 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 84)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,137 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,137 INFO scheduler.DAGScheduler: Submitting ResultStage 85 (MapPartitionsRDD[334] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,142 INFO memory.MemoryStore: Block broadcast_68 stored as values in memory (estimated size 179.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,143 INFO memory.MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,143 INFO storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on 10.0.117.124:39417 (size: 49.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,144 INFO spark.SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,144 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 85 (MapPartitionsRDD[334] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,144 INFO cluster.YarnScheduler: Adding task set 85.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,145 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 85.0 (TID 66) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,153 INFO storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on algo-1:33575 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,162 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 27 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,245 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 85.0 (TID 66) in 100 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,245 INFO cluster.YarnScheduler: Removed TaskSet 85.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,246 INFO scheduler.DAGScheduler: ResultStage 85 (collect at AnalysisRunner.scala:326) finished in 0.109 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,246 INFO scheduler.DAGScheduler: Job 57 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,246 INFO cluster.YarnScheduler: Killing all running tasks in stage 85: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,246 INFO scheduler.DAGScheduler: Job 57 finished: collect at AnalysisRunner.scala:326, took 0.111572 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,341 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,342 INFO scheduler.DAGScheduler: Got job 58 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,342 INFO scheduler.DAGScheduler: Final stage: ResultStage 86 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,342 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,343 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,343 INFO scheduler.DAGScheduler: Submitting ResultStage 86 (MapPartitionsRDD[344] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,349 INFO memory.MemoryStore: Block broadcast_69 stored as values in memory (estimated size 48.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,351 INFO memory.MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,351 INFO storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on 10.0.117.124:39417 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,352 INFO spark.SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,352 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 86 (MapPartitionsRDD[344] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,352 INFO cluster.YarnScheduler: Adding task set 86.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,353 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 86.0 (TID 67) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,364 INFO storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on algo-1:33575 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,406 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 86.0 (TID 67) in 53 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,406 INFO cluster.YarnScheduler: Removed TaskSet 86.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,407 INFO scheduler.DAGScheduler: ResultStage 86 (treeReduce at KLLRunner.scala:107) finished in 0.063 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,407 INFO scheduler.DAGScheduler: Job 58 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,407 INFO cluster.YarnScheduler: Killing all running tasks in stage 86: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,408 INFO scheduler.DAGScheduler: Job 58 finished: treeReduce at KLLRunner.scala:107, took 0.066546 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,515 INFO scheduler.DAGScheduler: Registering RDD 349 (collect at AnalysisRunner.scala:326) as input to shuffle 28\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,516 INFO scheduler.DAGScheduler: Got map stage job 59 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,516 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 87 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,516 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,517 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,517 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 87 (MapPartitionsRDD[349] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,520 INFO memory.MemoryStore: Block broadcast_70 stored as values in memory (estimated size 87.4 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,521 INFO memory.MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,521 INFO storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on 10.0.117.124:39417 (size: 27.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,522 INFO spark.SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,522 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 87 (MapPartitionsRDD[349] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,522 INFO cluster.YarnScheduler: Adding task set 87.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,523 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 87.0 (TID 68) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,530 INFO storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on algo-1:33575 (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,543 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 87.0 (TID 68) in 20 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,543 INFO cluster.YarnScheduler: Removed TaskSet 87.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,544 INFO scheduler.DAGScheduler: ShuffleMapStage 87 (collect at AnalysisRunner.scala:326) finished in 0.025 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,544 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,544 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,544 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,545 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,600 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,601 INFO scheduler.DAGScheduler: Got job 60 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,602 INFO scheduler.DAGScheduler: Final stage: ResultStage 89 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,602 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 88)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,602 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,603 INFO scheduler.DAGScheduler: Submitting ResultStage 89 (MapPartitionsRDD[352] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,604 INFO memory.MemoryStore: Block broadcast_71 stored as values in memory (estimated size 67.7 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,606 INFO memory.MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,606 INFO storage.BlockManagerInfo: Added broadcast_71_piece0 in memory on 10.0.117.124:39417 (size: 19.8 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,607 INFO spark.SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,607 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 89 (MapPartitionsRDD[352] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,607 INFO cluster.YarnScheduler: Adding task set 89.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,609 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 89.0 (TID 69) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,616 INFO storage.BlockManagerInfo: Added broadcast_71_piece0 in memory on algo-1:33575 (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,620 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 28 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,624 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 89.0 (TID 69) in 16 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,624 INFO cluster.YarnScheduler: Removed TaskSet 89.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,624 INFO scheduler.DAGScheduler: ResultStage 89 (collect at AnalysisRunner.scala:326) finished in 0.021 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,624 INFO scheduler.DAGScheduler: Job 60 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,624 INFO cluster.YarnScheduler: Killing all running tasks in stage 89: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,625 INFO scheduler.DAGScheduler: Job 60 finished: collect at AnalysisRunner.scala:326, took 0.023722 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,674 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,674 INFO scheduler.DAGScheduler: Registering RDD 360 (countByKey at ColumnProfiler.scala:592) as input to shuffle 29\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,675 INFO scheduler.DAGScheduler: Got job 61 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,675 INFO scheduler.DAGScheduler: Final stage: ResultStage 91 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,675 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 90)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,675 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 90)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,676 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 90 (MapPartitionsRDD[360] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,680 INFO memory.MemoryStore: Block broadcast_72 stored as values in memory (estimated size 41.8 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,682 INFO memory.MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,682 INFO storage.BlockManagerInfo: Added broadcast_72_piece0 in memory on 10.0.117.124:39417 (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,682 INFO spark.SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,683 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 90 (MapPartitionsRDD[360] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,683 INFO cluster.YarnScheduler: Adding task set 90.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,684 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 90.0 (TID 70) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,690 INFO storage.BlockManagerInfo: Added broadcast_72_piece0 in memory on algo-1:33575 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,707 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 90.0 (TID 70) in 23 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,707 INFO cluster.YarnScheduler: Removed TaskSet 90.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,707 INFO scheduler.DAGScheduler: ShuffleMapStage 90 (countByKey at ColumnProfiler.scala:592) finished in 0.031 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,708 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,709 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,709 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 91)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,709 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,709 INFO scheduler.DAGScheduler: Submitting ResultStage 91 (ShuffledRDD[361] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,711 INFO memory.MemoryStore: Block broadcast_73 stored as values in memory (estimated size 5.1 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,713 INFO memory.MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,714 INFO storage.BlockManagerInfo: Added broadcast_73_piece0 in memory on 10.0.117.124:39417 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,716 INFO spark.SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,716 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 91 (ShuffledRDD[361] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,716 INFO cluster.YarnScheduler: Adding task set 91.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,717 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 91.0 (TID 71) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,723 INFO storage.BlockManagerInfo: Added broadcast_73_piece0 in memory on algo-1:33575 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,728 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 29 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,735 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 91.0 (TID 71) in 18 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,735 INFO cluster.YarnScheduler: Removed TaskSet 91.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,735 INFO scheduler.DAGScheduler: ResultStage 91 (countByKey at ColumnProfiler.scala:592) finished in 0.025 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,736 INFO scheduler.DAGScheduler: Job 61 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,737 INFO cluster.YarnScheduler: Killing all running tasks in stage 91: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,737 INFO scheduler.DAGScheduler: Job 61 finished: countByKey at ColumnProfiler.scala:592, took 0.063297 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,809 INFO scheduler.DAGScheduler: Registering RDD 366 (collect at AnalysisRunner.scala:326) as input to shuffle 30\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,809 INFO scheduler.DAGScheduler: Got map stage job 62 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,809 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 92 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,809 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,809 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,809 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 92 (MapPartitionsRDD[366] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,813 INFO memory.MemoryStore: Block broadcast_74 stored as values in memory (estimated size 94.7 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,814 INFO memory.MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,815 INFO storage.BlockManagerInfo: Added broadcast_74_piece0 in memory on 10.0.117.124:39417 (size: 30.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,815 INFO spark.SparkContext: Created broadcast 74 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,815 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 92 (MapPartitionsRDD[366] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,815 INFO cluster.YarnScheduler: Adding task set 92.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,816 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 92.0 (TID 72) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,824 INFO storage.BlockManagerInfo: Added broadcast_74_piece0 in memory on algo-1:33575 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,896 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 92.0 (TID 72) in 80 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,896 INFO cluster.YarnScheduler: Removed TaskSet 92.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,897 INFO scheduler.DAGScheduler: ShuffleMapStage 92 (collect at AnalysisRunner.scala:326) finished in 0.086 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,897 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,897 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,897 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,897 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,932 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,932 INFO scheduler.DAGScheduler: Got job 63 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,933 INFO scheduler.DAGScheduler: Final stage: ResultStage 94 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,933 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 93)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,933 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,933 INFO scheduler.DAGScheduler: Submitting ResultStage 94 (MapPartitionsRDD[369] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,940 INFO memory.MemoryStore: Block broadcast_75 stored as values in memory (estimated size 179.5 KiB, free 1456.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,943 INFO memory.MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1456.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,943 INFO storage.BlockManagerInfo: Added broadcast_75_piece0 in memory on 10.0.117.124:39417 (size: 49.2 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,943 INFO spark.SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,944 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 94 (MapPartitionsRDD[369] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,944 INFO cluster.YarnScheduler: Adding task set 94.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,945 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 94.0 (TID 73) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,953 INFO storage.BlockManagerInfo: Added broadcast_75_piece0 in memory on algo-1:33575 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:15,964 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 30 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,094 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 94.0 (TID 73) in 149 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,094 INFO cluster.YarnScheduler: Removed TaskSet 94.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,095 INFO scheduler.DAGScheduler: ResultStage 94 (collect at AnalysisRunner.scala:326) finished in 0.161 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,095 INFO scheduler.DAGScheduler: Job 63 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,096 INFO cluster.YarnScheduler: Killing all running tasks in stage 94: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,097 INFO scheduler.DAGScheduler: Job 63 finished: collect at AnalysisRunner.scala:326, took 0.164877 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,212 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,213 INFO scheduler.DAGScheduler: Got job 64 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,214 INFO scheduler.DAGScheduler: Final stage: ResultStage 95 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,214 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,214 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,215 INFO scheduler.DAGScheduler: Submitting ResultStage 95 (MapPartitionsRDD[379] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,219 INFO memory.MemoryStore: Block broadcast_76 stored as values in memory (estimated size 48.9 KiB, free 1456.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,221 INFO memory.MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1456.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,221 INFO storage.BlockManagerInfo: Added broadcast_76_piece0 in memory on 10.0.117.124:39417 (size: 19.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,222 INFO spark.SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,222 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 95 (MapPartitionsRDD[379] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,222 INFO cluster.YarnScheduler: Adding task set 95.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,226 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 95.0 (TID 74) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,235 INFO storage.BlockManagerInfo: Added broadcast_76_piece0 in memory on algo-1:33575 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,268 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 95.0 (TID 74) in 42 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,268 INFO cluster.YarnScheduler: Removed TaskSet 95.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,268 INFO scheduler.DAGScheduler: ResultStage 95 (treeReduce at KLLRunner.scala:107) finished in 0.052 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,276 INFO scheduler.DAGScheduler: Job 64 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,276 INFO cluster.YarnScheduler: Killing all running tasks in stage 95: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,277 INFO scheduler.DAGScheduler: Job 64 finished: treeReduce at KLLRunner.scala:107, took 0.063941 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,414 INFO scheduler.DAGScheduler: Registering RDD 384 (collect at AnalysisRunner.scala:326) as input to shuffle 31\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,415 INFO scheduler.DAGScheduler: Got map stage job 65 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,415 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 96 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,415 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,416 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,416 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 96 (MapPartitionsRDD[384] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,419 INFO memory.MemoryStore: Block broadcast_77 stored as values in memory (estimated size 87.4 KiB, free 1456.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,422 INFO memory.MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 1456.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,423 INFO storage.BlockManagerInfo: Added broadcast_77_piece0 in memory on 10.0.117.124:39417 (size: 27.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,423 INFO spark.SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,424 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 96 (MapPartitionsRDD[384] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,424 INFO cluster.YarnScheduler: Adding task set 96.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,426 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 96.0 (TID 75) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,433 INFO storage.BlockManagerInfo: Added broadcast_77_piece0 in memory on algo-1:33575 (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,444 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 96.0 (TID 75) in 18 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,444 INFO cluster.YarnScheduler: Removed TaskSet 96.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,446 INFO scheduler.DAGScheduler: ShuffleMapStage 96 (collect at AnalysisRunner.scala:326) finished in 0.029 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,446 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,446 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,446 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,447 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,492 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,493 INFO scheduler.DAGScheduler: Got job 66 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,494 INFO scheduler.DAGScheduler: Final stage: ResultStage 98 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,494 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 97)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,495 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,495 INFO scheduler.DAGScheduler: Submitting ResultStage 98 (MapPartitionsRDD[387] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,497 INFO memory.MemoryStore: Block broadcast_78 stored as values in memory (estimated size 67.7 KiB, free 1456.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,500 INFO memory.MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 1456.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,501 INFO storage.BlockManagerInfo: Added broadcast_78_piece0 in memory on 10.0.117.124:39417 (size: 19.8 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,501 INFO spark.SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,502 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 98 (MapPartitionsRDD[387] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,502 INFO cluster.YarnScheduler: Adding task set 98.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,503 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 98.0 (TID 76) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,515 INFO storage.BlockManagerInfo: Added broadcast_78_piece0 in memory on algo-1:33575 (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,521 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 31 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,531 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 98.0 (TID 76) in 28 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,531 INFO cluster.YarnScheduler: Removed TaskSet 98.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,531 INFO scheduler.DAGScheduler: ResultStage 98 (collect at AnalysisRunner.scala:326) finished in 0.035 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,531 INFO scheduler.DAGScheduler: Job 66 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,531 INFO cluster.YarnScheduler: Killing all running tasks in stage 98: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,532 INFO scheduler.DAGScheduler: Job 66 finished: collect at AnalysisRunner.scala:326, took 0.039387 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,638 INFO storage.BlockManagerInfo: Removed broadcast_72_piece0 on 10.0.117.124:39417 in memory (size: 17.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,644 INFO storage.BlockManagerInfo: Removed broadcast_72_piece0 on algo-1:33575 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,648 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,649 INFO storage.BlockManagerInfo: Removed broadcast_73_piece0 on 10.0.117.124:39417 in memory (size: 3.0 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,650 INFO storage.BlockManagerInfo: Removed broadcast_73_piece0 on algo-1:33575 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,650 INFO scheduler.DAGScheduler: Registering RDD 395 (countByKey at ColumnProfiler.scala:592) as input to shuffle 32\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,651 INFO scheduler.DAGScheduler: Got job 67 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,651 INFO scheduler.DAGScheduler: Final stage: ResultStage 100 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,652 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 99)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,656 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 99)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,653 INFO storage.BlockManagerInfo: Removed broadcast_74_piece0 on 10.0.117.124:39417 in memory (size: 30.1 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,657 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 99 (MapPartitionsRDD[395] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,658 INFO storage.BlockManagerInfo: Removed broadcast_74_piece0 on algo-1:33575 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,662 INFO storage.BlockManagerInfo: Removed broadcast_78_piece0 on 10.0.117.124:39417 in memory (size: 19.8 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,663 INFO storage.BlockManagerInfo: Removed broadcast_78_piece0 on algo-1:33575 in memory (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,666 INFO memory.MemoryStore: Block broadcast_79 stored as values in memory (estimated size 41.8 KiB, free 1456.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,666 INFO storage.BlockManagerInfo: Removed broadcast_67_piece0 on 10.0.117.124:39417 in memory (size: 30.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,668 INFO storage.BlockManagerInfo: Removed broadcast_67_piece0 on algo-1:33575 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,670 INFO storage.BlockManagerInfo: Removed broadcast_76_piece0 on 10.0.117.124:39417 in memory (size: 19.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,671 INFO storage.BlockManagerInfo: Removed broadcast_76_piece0 on algo-1:33575 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,672 INFO memory.MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,674 INFO storage.BlockManagerInfo: Removed broadcast_68_piece0 on 10.0.117.124:39417 in memory (size: 49.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,676 INFO storage.BlockManagerInfo: Removed broadcast_68_piece0 on algo-1:33575 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,677 INFO storage.BlockManagerInfo: Added broadcast_79_piece0 in memory on 10.0.117.124:39417 (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,678 INFO spark.SparkContext: Created broadcast 79 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,679 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 99 (MapPartitionsRDD[395] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,680 INFO cluster.YarnScheduler: Adding task set 99.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,679 INFO storage.BlockManagerInfo: Removed broadcast_65_piece0 on 10.0.117.124:39417 in memory (size: 17.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,681 INFO storage.BlockManagerInfo: Removed broadcast_65_piece0 on algo-1:33575 in memory (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,685 INFO storage.BlockManagerInfo: Removed broadcast_66_piece0 on 10.0.117.124:39417 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,685 INFO storage.BlockManagerInfo: Removed broadcast_66_piece0 on algo-1:33575 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,687 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 99.0 (TID 77) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,688 INFO storage.BlockManagerInfo: Removed broadcast_71_piece0 on 10.0.117.124:39417 in memory (size: 19.8 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,690 INFO storage.BlockManagerInfo: Removed broadcast_71_piece0 on algo-1:33575 in memory (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,694 INFO storage.BlockManagerInfo: Removed broadcast_69_piece0 on 10.0.117.124:39417 in memory (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,696 INFO storage.BlockManagerInfo: Removed broadcast_69_piece0 on algo-1:33575 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,697 INFO storage.BlockManagerInfo: Added broadcast_79_piece0 in memory on algo-1:33575 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,712 INFO storage.BlockManagerInfo: Removed broadcast_77_piece0 on 10.0.117.124:39417 in memory (size: 27.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,714 INFO storage.BlockManagerInfo: Removed broadcast_77_piece0 on algo-1:33575 in memory (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,716 INFO storage.BlockManagerInfo: Removed broadcast_70_piece0 on 10.0.117.124:39417 in memory (size: 27.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,717 INFO storage.BlockManagerInfo: Removed broadcast_70_piece0 on algo-1:33575 in memory (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,721 INFO storage.BlockManagerInfo: Removed broadcast_64_piece0 on 10.0.117.124:39417 in memory (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,722 INFO storage.BlockManagerInfo: Removed broadcast_64_piece0 on algo-1:33575 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,724 INFO storage.BlockManagerInfo: Removed broadcast_75_piece0 on 10.0.117.124:39417 in memory (size: 49.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,725 INFO storage.BlockManagerInfo: Removed broadcast_75_piece0 on algo-1:33575 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,726 INFO storage.BlockManagerInfo: Removed broadcast_63_piece0 on 10.0.117.124:39417 in memory (size: 27.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,728 INFO storage.BlockManagerInfo: Removed broadcast_63_piece0 on algo-1:33575 in memory (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,743 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 99.0 (TID 77) in 56 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,743 INFO cluster.YarnScheduler: Removed TaskSet 99.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,744 INFO scheduler.DAGScheduler: ShuffleMapStage 99 (countByKey at ColumnProfiler.scala:592) finished in 0.086 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,744 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,744 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,745 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 100)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,745 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,745 INFO scheduler.DAGScheduler: Submitting ResultStage 100 (ShuffledRDD[396] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,747 INFO memory.MemoryStore: Block broadcast_80 stored as values in memory (estimated size 5.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,748 INFO memory.MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,748 INFO storage.BlockManagerInfo: Added broadcast_80_piece0 in memory on 10.0.117.124:39417 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,749 INFO spark.SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,749 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 100 (ShuffledRDD[396] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,749 INFO cluster.YarnScheduler: Adding task set 100.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,750 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 100.0 (TID 78) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,757 INFO storage.BlockManagerInfo: Added broadcast_80_piece0 in memory on algo-1:33575 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,759 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 32 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,770 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 100.0 (TID 78) in 21 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,771 INFO cluster.YarnScheduler: Removed TaskSet 100.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,771 INFO scheduler.DAGScheduler: ResultStage 100 (countByKey at ColumnProfiler.scala:592) finished in 0.025 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,771 INFO scheduler.DAGScheduler: Job 67 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,771 INFO cluster.YarnScheduler: Killing all running tasks in stage 100: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,771 INFO scheduler.DAGScheduler: Job 67 finished: countByKey at ColumnProfiler.scala:592, took 0.122807 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,891 INFO scheduler.DAGScheduler: Registering RDD 401 (collect at AnalysisRunner.scala:326) as input to shuffle 33\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,891 INFO scheduler.DAGScheduler: Got map stage job 68 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,891 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 101 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,891 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,892 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,892 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 101 (MapPartitionsRDD[401] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,895 INFO memory.MemoryStore: Block broadcast_81 stored as values in memory (estimated size 94.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,897 INFO memory.MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,898 INFO storage.BlockManagerInfo: Added broadcast_81_piece0 in memory on 10.0.117.124:39417 (size: 30.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,898 INFO spark.SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,899 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 101 (MapPartitionsRDD[401] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,899 INFO cluster.YarnScheduler: Adding task set 101.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,900 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 101.0 (TID 79) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:16,907 INFO storage.BlockManagerInfo: Added broadcast_81_piece0 in memory on algo-1:33575 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,008 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 101.0 (TID 79) in 108 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,008 INFO cluster.YarnScheduler: Removed TaskSet 101.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,009 INFO scheduler.DAGScheduler: ShuffleMapStage 101 (collect at AnalysisRunner.scala:326) finished in 0.117 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,010 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,010 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,011 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,011 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,042 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,043 INFO scheduler.DAGScheduler: Got job 69 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,043 INFO scheduler.DAGScheduler: Final stage: ResultStage 103 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,043 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 102)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,043 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,043 INFO scheduler.DAGScheduler: Submitting ResultStage 103 (MapPartitionsRDD[404] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,049 INFO memory.MemoryStore: Block broadcast_82 stored as values in memory (estimated size 179.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,050 INFO memory.MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 49.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,050 INFO storage.BlockManagerInfo: Added broadcast_82_piece0 in memory on 10.0.117.124:39417 (size: 49.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,051 INFO spark.SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,051 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 103 (MapPartitionsRDD[404] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,051 INFO cluster.YarnScheduler: Adding task set 103.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,052 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 103.0 (TID 80) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,058 INFO storage.BlockManagerInfo: Added broadcast_82_piece0 in memory on algo-1:33575 (size: 49.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,067 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 33 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,148 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 103.0 (TID 80) in 96 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,148 INFO cluster.YarnScheduler: Removed TaskSet 103.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,149 INFO scheduler.DAGScheduler: ResultStage 103 (collect at AnalysisRunner.scala:326) finished in 0.105 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,149 INFO scheduler.DAGScheduler: Job 69 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,149 INFO cluster.YarnScheduler: Killing all running tasks in stage 103: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,150 INFO scheduler.DAGScheduler: Job 69 finished: collect at AnalysisRunner.scala:326, took 0.107615 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,233 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,233 INFO scheduler.DAGScheduler: Got job 70 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,234 INFO scheduler.DAGScheduler: Final stage: ResultStage 104 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,234 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,234 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,234 INFO scheduler.DAGScheduler: Submitting ResultStage 104 (MapPartitionsRDD[414] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,242 INFO memory.MemoryStore: Block broadcast_83 stored as values in memory (estimated size 48.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,243 INFO memory.MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,243 INFO storage.BlockManagerInfo: Added broadcast_83_piece0 in memory on 10.0.117.124:39417 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,244 INFO spark.SparkContext: Created broadcast 83 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,244 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 104 (MapPartitionsRDD[414] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,244 INFO cluster.YarnScheduler: Adding task set 104.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,245 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 104.0 (TID 81) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,251 INFO storage.BlockManagerInfo: Added broadcast_83_piece0 in memory on algo-1:33575 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,282 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 104.0 (TID 81) in 37 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,282 INFO cluster.YarnScheduler: Removed TaskSet 104.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,283 INFO scheduler.DAGScheduler: ResultStage 104 (treeReduce at KLLRunner.scala:107) finished in 0.048 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,283 INFO scheduler.DAGScheduler: Job 70 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,283 INFO cluster.YarnScheduler: Killing all running tasks in stage 104: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,284 INFO scheduler.DAGScheduler: Job 70 finished: treeReduce at KLLRunner.scala:107, took 0.051100 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,381 INFO scheduler.DAGScheduler: Registering RDD 419 (collect at AnalysisRunner.scala:326) as input to shuffle 34\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,381 INFO scheduler.DAGScheduler: Got map stage job 71 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,381 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 105 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,381 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,381 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,382 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 105 (MapPartitionsRDD[419] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,384 INFO memory.MemoryStore: Block broadcast_84 stored as values in memory (estimated size 87.4 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,386 INFO memory.MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,386 INFO storage.BlockManagerInfo: Added broadcast_84_piece0 in memory on 10.0.117.124:39417 (size: 27.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,386 INFO spark.SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,387 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 105 (MapPartitionsRDD[419] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,387 INFO cluster.YarnScheduler: Adding task set 105.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,388 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 105.0 (TID 82) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,396 INFO storage.BlockManagerInfo: Added broadcast_84_piece0 in memory on algo-1:33575 (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,413 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 105.0 (TID 82) in 25 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,413 INFO cluster.YarnScheduler: Removed TaskSet 105.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,414 INFO scheduler.DAGScheduler: ShuffleMapStage 105 (collect at AnalysisRunner.scala:326) finished in 0.032 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,415 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,415 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,415 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,415 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,456 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,457 INFO scheduler.DAGScheduler: Got job 72 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,457 INFO scheduler.DAGScheduler: Final stage: ResultStage 107 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,457 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 106)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,457 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,458 INFO scheduler.DAGScheduler: Submitting ResultStage 107 (MapPartitionsRDD[422] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,459 INFO memory.MemoryStore: Block broadcast_85 stored as values in memory (estimated size 67.7 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,461 INFO memory.MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,461 INFO storage.BlockManagerInfo: Added broadcast_85_piece0 in memory on 10.0.117.124:39417 (size: 19.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,461 INFO spark.SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,461 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 107 (MapPartitionsRDD[422] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,461 INFO cluster.YarnScheduler: Adding task set 107.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,462 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 107.0 (TID 83) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,470 INFO storage.BlockManagerInfo: Added broadcast_85_piece0 in memory on algo-1:33575 (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,473 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 34 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,478 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 107.0 (TID 83) in 16 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,479 INFO cluster.YarnScheduler: Removed TaskSet 107.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,479 INFO scheduler.DAGScheduler: ResultStage 107 (collect at AnalysisRunner.scala:326) finished in 0.021 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,479 INFO scheduler.DAGScheduler: Job 72 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,479 INFO cluster.YarnScheduler: Killing all running tasks in stage 107: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,480 INFO scheduler.DAGScheduler: Job 72 finished: collect at AnalysisRunner.scala:326, took 0.024157 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,512 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,513 INFO scheduler.DAGScheduler: Registering RDD 430 (countByKey at ColumnProfiler.scala:592) as input to shuffle 35\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,513 INFO scheduler.DAGScheduler: Got job 73 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,513 INFO scheduler.DAGScheduler: Final stage: ResultStage 109 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,513 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 108)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,513 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 108)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,514 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 108 (MapPartitionsRDD[430] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,517 INFO memory.MemoryStore: Block broadcast_86 stored as values in memory (estimated size 41.8 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,519 INFO memory.MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,519 INFO storage.BlockManagerInfo: Added broadcast_86_piece0 in memory on 10.0.117.124:39417 (size: 17.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,519 INFO spark.SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,519 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 108 (MapPartitionsRDD[430] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,520 INFO cluster.YarnScheduler: Adding task set 108.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,521 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 108.0 (TID 84) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,527 INFO storage.BlockManagerInfo: Added broadcast_86_piece0 in memory on algo-1:33575 (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,546 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 108.0 (TID 84) in 25 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,548 INFO cluster.YarnScheduler: Removed TaskSet 108.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,549 INFO scheduler.DAGScheduler: ShuffleMapStage 108 (countByKey at ColumnProfiler.scala:592) finished in 0.034 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,549 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,549 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,549 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 109)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,549 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,550 INFO scheduler.DAGScheduler: Submitting ResultStage 109 (ShuffledRDD[431] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,551 INFO memory.MemoryStore: Block broadcast_87 stored as values in memory (estimated size 5.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,553 INFO memory.MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,553 INFO storage.BlockManagerInfo: Added broadcast_87_piece0 in memory on 10.0.117.124:39417 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,553 INFO spark.SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,553 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 109 (ShuffledRDD[431] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,553 INFO cluster.YarnScheduler: Adding task set 109.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,554 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 109.0 (TID 85) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,559 INFO storage.BlockManagerInfo: Added broadcast_87_piece0 in memory on algo-1:33575 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,562 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 35 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,569 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 109.0 (TID 85) in 15 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,569 INFO cluster.YarnScheduler: Removed TaskSet 109.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,569 INFO scheduler.DAGScheduler: ResultStage 109 (countByKey at ColumnProfiler.scala:592) finished in 0.019 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,570 INFO scheduler.DAGScheduler: Job 73 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,570 INFO cluster.YarnScheduler: Killing all running tasks in stage 109: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,570 INFO scheduler.DAGScheduler: Job 73 finished: countByKey at ColumnProfiler.scala:592, took 0.057389 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,650 INFO scheduler.DAGScheduler: Registering RDD 436 (collect at AnalysisRunner.scala:326) as input to shuffle 36\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,650 INFO scheduler.DAGScheduler: Got map stage job 74 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,650 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 110 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,650 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,651 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,652 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 110 (MapPartitionsRDD[436] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,656 INFO memory.MemoryStore: Block broadcast_88 stored as values in memory (estimated size 94.7 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,662 INFO memory.MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,663 INFO storage.BlockManagerInfo: Added broadcast_88_piece0 in memory on 10.0.117.124:39417 (size: 30.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,664 INFO spark.SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,664 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 110 (MapPartitionsRDD[436] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,664 INFO cluster.YarnScheduler: Adding task set 110.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,666 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 110.0 (TID 86) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,676 INFO storage.BlockManagerInfo: Added broadcast_88_piece0 in memory on algo-1:33575 (size: 30.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,807 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 110.0 (TID 86) in 141 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,807 INFO cluster.YarnScheduler: Removed TaskSet 110.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,808 INFO scheduler.DAGScheduler: ShuffleMapStage 110 (collect at AnalysisRunner.scala:326) finished in 0.155 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,808 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,808 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,808 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,808 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,838 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,839 INFO scheduler.DAGScheduler: Got job 75 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,839 INFO scheduler.DAGScheduler: Final stage: ResultStage 112 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,839 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 111)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,839 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,839 INFO scheduler.DAGScheduler: Submitting ResultStage 112 (MapPartitionsRDD[439] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,845 INFO memory.MemoryStore: Block broadcast_89 stored as values in memory (estimated size 179.6 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,847 INFO memory.MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,847 INFO storage.BlockManagerInfo: Added broadcast_89_piece0 in memory on 10.0.117.124:39417 (size: 49.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,847 INFO spark.SparkContext: Created broadcast 89 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,847 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 112 (MapPartitionsRDD[439] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,847 INFO cluster.YarnScheduler: Adding task set 112.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,848 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 112.0 (TID 87) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,854 INFO storage.BlockManagerInfo: Added broadcast_89_piece0 in memory on algo-1:33575 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,862 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 36 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,921 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 112.0 (TID 87) in 73 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,921 INFO cluster.YarnScheduler: Removed TaskSet 112.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,921 INFO scheduler.DAGScheduler: ResultStage 112 (collect at AnalysisRunner.scala:326) finished in 0.081 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,922 INFO scheduler.DAGScheduler: Job 75 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,922 INFO cluster.YarnScheduler: Killing all running tasks in stage 112: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,923 INFO scheduler.DAGScheduler: Job 75 finished: collect at AnalysisRunner.scala:326, took 0.084747 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:17,980 INFO codegen.CodeGenerator: Code generated in 9.869271 ms\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,002 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,003 INFO scheduler.DAGScheduler: Got job 76 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,003 INFO scheduler.DAGScheduler: Final stage: ResultStage 113 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,003 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,003 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,003 INFO scheduler.DAGScheduler: Submitting ResultStage 113 (MapPartitionsRDD[449] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,007 INFO memory.MemoryStore: Block broadcast_90 stored as values in memory (estimated size 48.9 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,008 INFO memory.MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,008 INFO storage.BlockManagerInfo: Added broadcast_90_piece0 in memory on 10.0.117.124:39417 (size: 19.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,009 INFO spark.SparkContext: Created broadcast 90 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,009 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 113 (MapPartitionsRDD[449] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,009 INFO cluster.YarnScheduler: Adding task set 113.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,010 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 113.0 (TID 88) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,015 INFO storage.BlockManagerInfo: Added broadcast_90_piece0 in memory on algo-1:33575 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,080 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 113.0 (TID 88) in 70 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,080 INFO cluster.YarnScheduler: Removed TaskSet 113.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,081 INFO scheduler.DAGScheduler: ResultStage 113 (treeReduce at KLLRunner.scala:107) finished in 0.077 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,081 INFO scheduler.DAGScheduler: Job 76 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,081 INFO cluster.YarnScheduler: Killing all running tasks in stage 113: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,082 INFO scheduler.DAGScheduler: Job 76 finished: treeReduce at KLLRunner.scala:107, took 0.079046 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,187 INFO codegen.CodeGenerator: Code generated in 31.343971 ms\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,192 INFO scheduler.DAGScheduler: Registering RDD 454 (collect at AnalysisRunner.scala:326) as input to shuffle 37\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,192 INFO scheduler.DAGScheduler: Got map stage job 77 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,192 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 114 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,192 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,193 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,193 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 114 (MapPartitionsRDD[454] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,196 INFO memory.MemoryStore: Block broadcast_91 stored as values in memory (estimated size 87.4 KiB, free 1456.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,197 INFO memory.MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 1456.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,197 INFO storage.BlockManagerInfo: Added broadcast_91_piece0 in memory on 10.0.117.124:39417 (size: 27.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,198 INFO spark.SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,198 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 114 (MapPartitionsRDD[454] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,198 INFO cluster.YarnScheduler: Adding task set 114.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,199 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 114.0 (TID 89) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,205 INFO storage.BlockManagerInfo: Added broadcast_91_piece0 in memory on algo-1:33575 (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,366 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 114.0 (TID 89) in 167 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,367 INFO cluster.YarnScheduler: Removed TaskSet 114.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,367 INFO scheduler.DAGScheduler: ShuffleMapStage 114 (collect at AnalysisRunner.scala:326) finished in 0.173 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,368 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,368 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,368 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,368 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,432 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,437 INFO scheduler.DAGScheduler: Got job 78 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,437 INFO scheduler.DAGScheduler: Final stage: ResultStage 116 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,437 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 115)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,437 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,438 INFO scheduler.DAGScheduler: Submitting ResultStage 116 (MapPartitionsRDD[457] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,439 INFO memory.MemoryStore: Block broadcast_92 stored as values in memory (estimated size 67.7 KiB, free 1456.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,441 INFO memory.MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 1456.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,441 INFO storage.BlockManagerInfo: Added broadcast_92_piece0 in memory on 10.0.117.124:39417 (size: 19.8 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,442 INFO spark.SparkContext: Created broadcast 92 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,442 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 116 (MapPartitionsRDD[457] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,442 INFO cluster.YarnScheduler: Adding task set 116.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,443 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 116.0 (TID 90) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,454 INFO storage.BlockManagerInfo: Added broadcast_92_piece0 in memory on algo-1:33575 (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,458 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 37 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,464 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 116.0 (TID 90) in 21 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,465 INFO cluster.YarnScheduler: Removed TaskSet 116.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,466 INFO scheduler.DAGScheduler: ResultStage 116 (collect at AnalysisRunner.scala:326) finished in 0.028 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,466 INFO scheduler.DAGScheduler: Job 78 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,467 INFO cluster.YarnScheduler: Killing all running tasks in stage 116: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,469 INFO scheduler.DAGScheduler: Job 78 finished: collect at AnalysisRunner.scala:326, took 0.033286 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,513 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,514 INFO scheduler.DAGScheduler: Registering RDD 465 (countByKey at ColumnProfiler.scala:592) as input to shuffle 38\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,514 INFO scheduler.DAGScheduler: Got job 79 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,514 INFO scheduler.DAGScheduler: Final stage: ResultStage 118 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,514 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 117)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,514 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 117)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,515 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 117 (MapPartitionsRDD[465] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,518 INFO memory.MemoryStore: Block broadcast_93 stored as values in memory (estimated size 41.8 KiB, free 1456.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,519 INFO memory.MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1456.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,519 INFO storage.BlockManagerInfo: Added broadcast_93_piece0 in memory on 10.0.117.124:39417 (size: 17.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,520 INFO spark.SparkContext: Created broadcast 93 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,520 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 117 (MapPartitionsRDD[465] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,520 INFO cluster.YarnScheduler: Adding task set 117.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,522 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 117.0 (TID 91) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,530 INFO storage.BlockManagerInfo: Added broadcast_93_piece0 in memory on algo-1:33575 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,545 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 117.0 (TID 91) in 23 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,545 INFO cluster.YarnScheduler: Removed TaskSet 117.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,546 INFO scheduler.DAGScheduler: ShuffleMapStage 117 (countByKey at ColumnProfiler.scala:592) finished in 0.031 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,546 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,547 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,547 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 118)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,547 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,547 INFO scheduler.DAGScheduler: Submitting ResultStage 118 (ShuffledRDD[466] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,548 INFO memory.MemoryStore: Block broadcast_94 stored as values in memory (estimated size 5.1 KiB, free 1456.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,549 INFO memory.MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1456.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,550 INFO storage.BlockManagerInfo: Added broadcast_94_piece0 in memory on 10.0.117.124:39417 (size: 3.0 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,550 INFO spark.SparkContext: Created broadcast 94 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,551 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 118 (ShuffledRDD[466] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,551 INFO cluster.YarnScheduler: Adding task set 118.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,552 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 118.0 (TID 92) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,557 INFO storage.BlockManagerInfo: Added broadcast_94_piece0 in memory on algo-1:33575 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,560 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 38 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,566 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 118.0 (TID 92) in 15 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,567 INFO cluster.YarnScheduler: Removed TaskSet 118.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,567 INFO scheduler.DAGScheduler: ResultStage 118 (countByKey at ColumnProfiler.scala:592) finished in 0.019 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,567 INFO scheduler.DAGScheduler: Job 79 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,568 INFO cluster.YarnScheduler: Killing all running tasks in stage 118: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,568 INFO scheduler.DAGScheduler: Job 79 finished: countByKey at ColumnProfiler.scala:592, took 0.054620 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,621 INFO scheduler.DAGScheduler: Registering RDD 471 (collect at AnalysisRunner.scala:326) as input to shuffle 39\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,622 INFO scheduler.DAGScheduler: Got map stage job 80 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,622 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 119 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,622 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,622 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,622 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 119 (MapPartitionsRDD[471] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,626 INFO memory.MemoryStore: Block broadcast_95 stored as values in memory (estimated size 94.7 KiB, free 1456.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,627 INFO memory.MemoryStore: Block broadcast_95_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1456.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,627 INFO storage.BlockManagerInfo: Added broadcast_95_piece0 in memory on 10.0.117.124:39417 (size: 30.0 KiB, free: 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,628 INFO spark.SparkContext: Created broadcast 95 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,628 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 119 (MapPartitionsRDD[471] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,628 INFO cluster.YarnScheduler: Adding task set 119.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,630 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 119.0 (TID 93) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,639 INFO storage.BlockManagerInfo: Added broadcast_95_piece0 in memory on algo-1:33575 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,732 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 119.0 (TID 93) in 102 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,732 INFO cluster.YarnScheduler: Removed TaskSet 119.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,733 INFO scheduler.DAGScheduler: ShuffleMapStage 119 (collect at AnalysisRunner.scala:326) finished in 0.110 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,733 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,733 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,733 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,733 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,758 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,759 INFO scheduler.DAGScheduler: Got job 81 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,759 INFO scheduler.DAGScheduler: Final stage: ResultStage 121 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,759 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 120)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,759 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,759 INFO scheduler.DAGScheduler: Submitting ResultStage 121 (MapPartitionsRDD[474] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,763 INFO memory.MemoryStore: Block broadcast_96 stored as values in memory (estimated size 179.7 KiB, free 1456.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,764 INFO memory.MemoryStore: Block broadcast_96_piece0 stored as bytes in memory (estimated size 49.3 KiB, free 1455.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,765 INFO storage.BlockManagerInfo: Added broadcast_96_piece0 in memory on 10.0.117.124:39417 (size: 49.3 KiB, free: 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,765 INFO spark.SparkContext: Created broadcast 96 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,765 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 121 (MapPartitionsRDD[474] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,765 INFO cluster.YarnScheduler: Adding task set 121.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,766 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 121.0 (TID 94) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,771 INFO storage.BlockManagerInfo: Added broadcast_96_piece0 in memory on algo-1:33575 (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,780 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 39 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,829 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 121.0 (TID 94) in 63 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,829 INFO cluster.YarnScheduler: Removed TaskSet 121.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,830 INFO scheduler.DAGScheduler: ResultStage 121 (collect at AnalysisRunner.scala:326) finished in 0.070 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,830 INFO scheduler.DAGScheduler: Job 81 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,830 INFO cluster.YarnScheduler: Killing all running tasks in stage 121: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,830 INFO scheduler.DAGScheduler: Job 81 finished: collect at AnalysisRunner.scala:326, took 0.072022 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,926 INFO storage.BlockManagerInfo: Removed broadcast_96_piece0 on 10.0.117.124:39417 in memory (size: 49.3 KiB, free: 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,927 INFO storage.BlockManagerInfo: Removed broadcast_96_piece0 on algo-1:33575 in memory (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,930 INFO storage.BlockManagerInfo: Removed broadcast_90_piece0 on 10.0.117.124:39417 in memory (size: 19.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,930 INFO storage.BlockManagerInfo: Removed broadcast_90_piece0 on algo-1:33575 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,935 INFO storage.BlockManagerInfo: Removed broadcast_80_piece0 on 10.0.117.124:39417 in memory (size: 3.0 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,935 INFO storage.BlockManagerInfo: Removed broadcast_80_piece0 on algo-1:33575 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,937 INFO storage.BlockManagerInfo: Removed broadcast_91_piece0 on 10.0.117.124:39417 in memory (size: 27.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,939 INFO storage.BlockManagerInfo: Removed broadcast_91_piece0 on algo-1:33575 in memory (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,940 INFO storage.BlockManagerInfo: Removed broadcast_82_piece0 on 10.0.117.124:39417 in memory (size: 49.1 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,941 INFO storage.BlockManagerInfo: Removed broadcast_82_piece0 on algo-1:33575 in memory (size: 49.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,943 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,943 INFO scheduler.DAGScheduler: Got job 82 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,943 INFO scheduler.DAGScheduler: Final stage: ResultStage 122 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,944 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,944 INFO storage.BlockManagerInfo: Removed broadcast_94_piece0 on 10.0.117.124:39417 in memory (size: 3.0 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,944 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,945 INFO scheduler.DAGScheduler: Submitting ResultStage 122 (MapPartitionsRDD[484] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,945 INFO storage.BlockManagerInfo: Removed broadcast_94_piece0 on algo-1:33575 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,948 INFO storage.BlockManagerInfo: Removed broadcast_84_piece0 on 10.0.117.124:39417 in memory (size: 27.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,950 INFO storage.BlockManagerInfo: Removed broadcast_84_piece0 on algo-1:33575 in memory (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,951 INFO memory.MemoryStore: Block broadcast_97 stored as values in memory (estimated size 48.9 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,952 INFO memory.MemoryStore: Block broadcast_97_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,952 INFO storage.BlockManagerInfo: Added broadcast_97_piece0 in memory on 10.0.117.124:39417 (size: 19.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,953 INFO spark.SparkContext: Created broadcast 97 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,953 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 122 (MapPartitionsRDD[484] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,953 INFO cluster.YarnScheduler: Adding task set 122.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,954 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 122.0 (TID 95) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,959 INFO storage.BlockManagerInfo: Removed broadcast_85_piece0 on 10.0.117.124:39417 in memory (size: 19.8 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,960 INFO storage.BlockManagerInfo: Removed broadcast_85_piece0 on algo-1:33575 in memory (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,963 INFO storage.BlockManagerInfo: Removed broadcast_95_piece0 on 10.0.117.124:39417 in memory (size: 30.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,964 INFO storage.BlockManagerInfo: Added broadcast_97_piece0 in memory on algo-1:33575 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,964 INFO storage.BlockManagerInfo: Removed broadcast_95_piece0 on algo-1:33575 in memory (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,966 INFO storage.BlockManagerInfo: Removed broadcast_89_piece0 on 10.0.117.124:39417 in memory (size: 49.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,967 INFO storage.BlockManagerInfo: Removed broadcast_89_piece0 on algo-1:33575 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,970 INFO storage.BlockManagerInfo: Removed broadcast_86_piece0 on 10.0.117.124:39417 in memory (size: 17.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,971 INFO storage.BlockManagerInfo: Removed broadcast_86_piece0 on algo-1:33575 in memory (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,972 INFO storage.BlockManagerInfo: Removed broadcast_79_piece0 on 10.0.117.124:39417 in memory (size: 17.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,973 INFO storage.BlockManagerInfo: Removed broadcast_79_piece0 on algo-1:33575 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,975 INFO storage.BlockManagerInfo: Removed broadcast_92_piece0 on 10.0.117.124:39417 in memory (size: 19.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,975 INFO storage.BlockManagerInfo: Removed broadcast_92_piece0 on algo-1:33575 in memory (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,978 INFO storage.BlockManagerInfo: Removed broadcast_83_piece0 on 10.0.117.124:39417 in memory (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,979 INFO storage.BlockManagerInfo: Removed broadcast_83_piece0 on algo-1:33575 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,982 INFO storage.BlockManagerInfo: Removed broadcast_88_piece0 on 10.0.117.124:39417 in memory (size: 30.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,983 INFO storage.BlockManagerInfo: Removed broadcast_88_piece0 on algo-1:33575 in memory (size: 30.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,985 INFO storage.BlockManagerInfo: Removed broadcast_87_piece0 on 10.0.117.124:39417 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,987 INFO storage.BlockManagerInfo: Removed broadcast_87_piece0 on algo-1:33575 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,992 INFO storage.BlockManagerInfo: Removed broadcast_81_piece0 on 10.0.117.124:39417 in memory (size: 30.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:18,999 INFO storage.BlockManagerInfo: Removed broadcast_81_piece0 on algo-1:33575 in memory (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,001 INFO storage.BlockManagerInfo: Removed broadcast_93_piece0 on 10.0.117.124:39417 in memory (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,002 INFO storage.BlockManagerInfo: Removed broadcast_93_piece0 on algo-1:33575 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,010 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 122.0 (TID 95) in 56 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,010 INFO scheduler.DAGScheduler: ResultStage 122 (treeReduce at KLLRunner.scala:107) finished in 0.065 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,010 INFO scheduler.DAGScheduler: Job 82 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,012 INFO cluster.YarnScheduler: Removed TaskSet 122.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,012 INFO cluster.YarnScheduler: Killing all running tasks in stage 122: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,012 INFO scheduler.DAGScheduler: Job 82 finished: treeReduce at KLLRunner.scala:107, took 0.069213 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,099 INFO scheduler.DAGScheduler: Registering RDD 489 (collect at AnalysisRunner.scala:326) as input to shuffle 40\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,099 INFO scheduler.DAGScheduler: Got map stage job 83 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,099 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 123 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,099 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,099 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,100 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 123 (MapPartitionsRDD[489] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,102 INFO memory.MemoryStore: Block broadcast_98 stored as values in memory (estimated size 87.4 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,103 INFO memory.MemoryStore: Block broadcast_98_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,104 INFO storage.BlockManagerInfo: Added broadcast_98_piece0 in memory on 10.0.117.124:39417 (size: 27.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,104 INFO spark.SparkContext: Created broadcast 98 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,104 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 123 (MapPartitionsRDD[489] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,104 INFO cluster.YarnScheduler: Adding task set 123.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,105 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 123.0 (TID 96) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,112 INFO storage.BlockManagerInfo: Added broadcast_98_piece0 in memory on algo-1:33575 (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,127 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 123.0 (TID 96) in 22 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,128 INFO cluster.YarnScheduler: Removed TaskSet 123.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,128 INFO scheduler.DAGScheduler: ShuffleMapStage 123 (collect at AnalysisRunner.scala:326) finished in 0.028 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,128 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,129 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,129 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,136 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,168 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,169 INFO scheduler.DAGScheduler: Got job 84 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,169 INFO scheduler.DAGScheduler: Final stage: ResultStage 125 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,169 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 124)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,169 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,169 INFO scheduler.DAGScheduler: Submitting ResultStage 125 (MapPartitionsRDD[492] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,170 INFO memory.MemoryStore: Block broadcast_99 stored as values in memory (estimated size 67.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,171 INFO memory.MemoryStore: Block broadcast_99_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,171 INFO storage.BlockManagerInfo: Added broadcast_99_piece0 in memory on 10.0.117.124:39417 (size: 19.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,172 INFO spark.SparkContext: Created broadcast 99 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,172 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 125 (MapPartitionsRDD[492] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,172 INFO cluster.YarnScheduler: Adding task set 125.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,173 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 125.0 (TID 97) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,178 INFO storage.BlockManagerInfo: Added broadcast_99_piece0 in memory on algo-1:33575 (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,180 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 40 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,183 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 125.0 (TID 97) in 10 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,184 INFO cluster.YarnScheduler: Removed TaskSet 125.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,184 INFO scheduler.DAGScheduler: ResultStage 125 (collect at AnalysisRunner.scala:326) finished in 0.015 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,184 INFO scheduler.DAGScheduler: Job 84 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,184 INFO cluster.YarnScheduler: Killing all running tasks in stage 125: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,184 INFO scheduler.DAGScheduler: Job 84 finished: collect at AnalysisRunner.scala:326, took 0.016183 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,217 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,217 INFO scheduler.DAGScheduler: Registering RDD 500 (countByKey at ColumnProfiler.scala:592) as input to shuffle 41\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,217 INFO scheduler.DAGScheduler: Got job 85 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,217 INFO scheduler.DAGScheduler: Final stage: ResultStage 127 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,218 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 126)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,218 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 126)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,218 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 126 (MapPartitionsRDD[500] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,221 INFO memory.MemoryStore: Block broadcast_100 stored as values in memory (estimated size 41.8 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,222 INFO memory.MemoryStore: Block broadcast_100_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,222 INFO storage.BlockManagerInfo: Added broadcast_100_piece0 in memory on 10.0.117.124:39417 (size: 17.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,223 INFO spark.SparkContext: Created broadcast 100 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,223 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 126 (MapPartitionsRDD[500] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,223 INFO cluster.YarnScheduler: Adding task set 126.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,224 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 126.0 (TID 98) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,229 INFO storage.BlockManagerInfo: Added broadcast_100_piece0 in memory on algo-1:33575 (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,242 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 126.0 (TID 98) in 18 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,242 INFO cluster.YarnScheduler: Removed TaskSet 126.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,242 INFO scheduler.DAGScheduler: ShuffleMapStage 126 (countByKey at ColumnProfiler.scala:592) finished in 0.023 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,242 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,243 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,243 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 127)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,243 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,243 INFO scheduler.DAGScheduler: Submitting ResultStage 127 (ShuffledRDD[501] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,244 INFO memory.MemoryStore: Block broadcast_101 stored as values in memory (estimated size 5.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,245 INFO memory.MemoryStore: Block broadcast_101_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,245 INFO storage.BlockManagerInfo: Added broadcast_101_piece0 in memory on 10.0.117.124:39417 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,245 INFO spark.SparkContext: Created broadcast 101 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,245 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 127 (ShuffledRDD[501] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,246 INFO cluster.YarnScheduler: Adding task set 127.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,246 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 127.0 (TID 99) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,251 INFO storage.BlockManagerInfo: Added broadcast_101_piece0 in memory on algo-1:33575 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,253 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 41 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,258 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 127.0 (TID 99) in 12 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,258 INFO cluster.YarnScheduler: Removed TaskSet 127.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,260 INFO scheduler.DAGScheduler: ResultStage 127 (countByKey at ColumnProfiler.scala:592) finished in 0.017 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,261 INFO scheduler.DAGScheduler: Job 85 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,261 INFO cluster.YarnScheduler: Killing all running tasks in stage 127: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,261 INFO scheduler.DAGScheduler: Job 85 finished: countByKey at ColumnProfiler.scala:592, took 0.044112 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,458 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,489 INFO codegen.CodeGenerator: Code generated in 8.287461 ms\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,494 INFO scheduler.DAGScheduler: Registering RDD 506 (count at StatsGenerator.scala:66) as input to shuffle 42\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,495 INFO scheduler.DAGScheduler: Got map stage job 86 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,495 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 128 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,495 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,496 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,496 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 128 (MapPartitionsRDD[506] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,499 INFO memory.MemoryStore: Block broadcast_102 stored as values in memory (estimated size 34.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,500 INFO memory.MemoryStore: Block broadcast_102_piece0 stored as bytes in memory (estimated size 13.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,501 INFO storage.BlockManagerInfo: Added broadcast_102_piece0 in memory on 10.0.117.124:39417 (size: 13.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,501 INFO spark.SparkContext: Created broadcast 102 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,501 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 128 (MapPartitionsRDD[506] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,501 INFO cluster.YarnScheduler: Adding task set 128.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,502 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 128.0 (TID 100) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,511 INFO storage.BlockManagerInfo: Added broadcast_102_piece0 in memory on algo-1:33575 (size: 13.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,552 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 128.0 (TID 100) in 50 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,552 INFO cluster.YarnScheduler: Removed TaskSet 128.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,553 INFO scheduler.DAGScheduler: ShuffleMapStage 128 (count at StatsGenerator.scala:66) finished in 0.056 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,554 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,554 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,554 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,554 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,569 INFO codegen.CodeGenerator: Code generated in 10.658582 ms\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,583 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,583 INFO scheduler.DAGScheduler: Got job 87 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,583 INFO scheduler.DAGScheduler: Final stage: ResultStage 130 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,583 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 129)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,583 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,584 INFO scheduler.DAGScheduler: Submitting ResultStage 130 (MapPartitionsRDD[509] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,585 INFO memory.MemoryStore: Block broadcast_103 stored as values in memory (estimated size 11.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,586 INFO memory.MemoryStore: Block broadcast_103_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,586 INFO storage.BlockManagerInfo: Added broadcast_103_piece0 in memory on 10.0.117.124:39417 (size: 5.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,587 INFO spark.SparkContext: Created broadcast 103 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,587 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 130 (MapPartitionsRDD[509] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,587 INFO cluster.YarnScheduler: Adding task set 130.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,588 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 130.0 (TID 101) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,594 INFO storage.BlockManagerInfo: Added broadcast_103_piece0 in memory on algo-1:33575 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,597 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 42 to 10.0.117.124:47078\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,609 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 130.0 (TID 101) in 21 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,609 INFO cluster.YarnScheduler: Removed TaskSet 130.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,610 INFO scheduler.DAGScheduler: ResultStage 130 (count at StatsGenerator.scala:66) finished in 0.025 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,610 INFO scheduler.DAGScheduler: Job 87 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,610 INFO cluster.YarnScheduler: Killing all running tasks in stage 130: Stage finished\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:19,611 INFO scheduler.DAGScheduler: Job 87 finished: count at StatsGenerator.scala:66, took 0.027917 s\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:20,295 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:20,330 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:20,394 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:20,395 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:20,411 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:20,439 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:20,503 WARN nio.NioEventLoop: Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@58c59b6.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:20,503 INFO nio.NioEventLoop: Migrated 0 channel(s) to the new Selector.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:20,531 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:20,531 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:20,535 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:20,542 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:20,620 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:20,620 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:20,620 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:20,669 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:20,670 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-68b02992-c720-4c38-8632-22c70c59f942\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:20,682 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-7940c544-7071-464f-845c-12d1fe581960\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:20,782 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2024-07-22 20:50:20,783 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.processing.ProcessingJob at 0x7f538d7ebc10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_uri + \"/training-dataset-with-header.csv\",\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8b2a3d",
   "metadata": {
    "papermill": {
     "duration": 0.048105,
     "end_time": "2022-04-18T00:29:46.442994",
     "exception": false,
     "start_time": "2022-04-18T00:29:46.394889",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Explore the generated constraints and statistics\n",
    "\n",
    "생성된 기준선 통계 및 제약 조건 탐색\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5298eea2",
   "metadata": {
    "papermill": {
     "duration": 0.227209,
     "end_time": "2022-04-18T00:29:46.718320",
     "exception": false,
     "start_time": "2022-04-18T00:29:46.491111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Files:\n",
      "sagemaker/DEMO-ModelMonitor/baselining/results/constraints.json\n",
      " sagemaker/DEMO-ModelMonitor/baselining/results/statistics.json\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.Session().client(\"s3\")\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=baseline_results_prefix)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get(\"Contents\")]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6179805",
   "metadata": {
    "papermill": {
     "duration": 0.444019,
     "end_time": "2022-04-18T00:29:47.210306",
     "exception": false,
     "start_time": "2022-04-18T00:29:46.766287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 데이터 처리 및 분석을 위해 pandas 라이브러리를 가져옵니다.\n",
    "import pandas as pd \n",
    "\n",
    "# 최근의 기준선 작업(baselining_job)을 가져옵니다. \n",
    "baseline_job = my_default_monitor.latest_baselining_job \n",
    "schema_df = pd.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d25691b9",
   "metadata": {
    "papermill": {
     "duration": 0.132679,
     "end_time": "2022-04-18T00:29:47.392401",
     "exception": false,
     "start_time": "2022-04-18T00:29:47.259722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>completeness</th>\n",
       "      <th>num_constraints.is_non_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Churn</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Account Length</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VMail Message</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Day Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Day Calls</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Eve Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Eve Calls</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Night Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Night Calls</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Intl Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name inferred_type  completeness  num_constraints.is_non_negative\n",
       "0           Churn      Integral           1.0                             True\n",
       "1  Account Length      Integral           1.0                             True\n",
       "2   VMail Message      Integral           1.0                             True\n",
       "3        Day Mins    Fractional           1.0                             True\n",
       "4       Day Calls      Integral           1.0                             True\n",
       "5        Eve Mins    Fractional           1.0                             True\n",
       "6       Eve Calls      Integral           1.0                             True\n",
       "7      Night Mins    Fractional           1.0                             True\n",
       "8     Night Calls      Integral           1.0                             True\n",
       "9       Intl Mins    Fractional           1.0                             True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "constraints_df = pd.json_normalize(\n",
    "    baseline_job.suggested_constraints().body_dict[\"features\"]\n",
    ")\n",
    "constraints_df.head(10) # 생성된 데이터프레임 constraints_df의 상위 10개 행을 출력합니다. 이로써 제안된 제약 조건의 구조와 내용을 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df1fea8",
   "metadata": {
    "papermill": {
     "duration": 0.049354,
     "end_time": "2022-04-18T00:29:47.491527",
     "exception": false,
     "start_time": "2022-04-18T00:29:47.442173",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. Analyze collected data for data quality issues\n",
    "\n",
    "When you have collected the data above, analyze and monitor the data with Monitoring Schedules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61d11a9",
   "metadata": {
    "papermill": {
     "duration": 0.049273,
     "end_time": "2022-04-18T00:29:47.590172",
     "exception": false,
     "start_time": "2022-04-18T00:29:47.540899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Create a schedule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc1c301f",
   "metadata": {
    "papermill": {
     "duration": 0.267475,
     "end_time": "2022-04-18T00:29:47.906709",
     "exception": false,
     "start_time": "2022-04-18T00:29:47.639234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload some test scripts to the S3 bucket for pre- and post-processing\n",
    "bucket = boto3.Session().resource(\"s3\").Bucket(bucket)\n",
    "bucket.Object(code_prefix + \"/preprocessor.py\").upload_file(\"preprocessor.py\")\n",
    "bucket.Object(code_prefix + \"/postprocessor.py\").upload_file(\"postprocessor.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e142c2be",
   "metadata": {
    "papermill": {
     "duration": 0.050333,
     "end_time": "2022-04-18T00:29:48.008206",
     "exception": false,
     "start_time": "2022-04-18T00:29:47.957873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You can create a model monitoring schedule for the endpoint created earlier. Use the baseline resources (constraints and statistics) to compare against the realtime traffic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7889de1",
   "metadata": {
    "papermill": {
     "duration": 0.667743,
     "end_time": "2022-04-18T00:29:48.725674",
     "exception": false,
     "start_time": "2022-04-18T00:29:48.057931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: DEMO-xgb-churn-pred-model-monitor-schedule-2024-07-22-20-55-19\n"
     ]
    }
   ],
   "source": [
    "# CronExpressionGenerator 모듈을 sagemaker.model_monitor에서 임포트합니다. \n",
    "# 이 모듈은 CloudWatch 이벤트를 사용하여 주기적으로 모니터링 작업을 실행하는 크론 표현식을 생성하는 데 사용됩니다.\n",
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "\n",
    "mon_schedule_name = \"DEMO-xgb-churn-pred-model-monitor-schedule-\" + strftime(\n",
    "    \"%Y-%m-%d-%H-%M-%S\", gmtime()\n",
    ")\n",
    "\n",
    "# my_default_monitor 객체를 사용하여 모델 모니터링 스케줄을 생성합니다. \n",
    "# create_monitoring_schedule로 주기적으로 모델의 성능을 모니터링하고 결과를 S3 버킷에 저장합니다.\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=predictor.endpoint, # 모니터링할 모델의 엔드포인트를 지정합니다. \n",
    "    # record_preprocessor_script=pre_processor_script,\n",
    "    post_analytics_processor_script=s3_code_postprocessor_uri, # 분석 후 처리 스크립트의 S3 URI를 지정합니다. 이 스크립트는 모니터링 결과를 후처리하는 데 사용됩니다.\n",
    "    output_s3_uri=s3_report_path,\n",
    "    statistics=my_default_monitor.baseline_statistics(), # 기준선 통계를 지정해서 이 통계는 모델의 정상 동작 범위를 정의합니다.\n",
    "    constraints=my_default_monitor.suggested_constraints(), # 제안된 제약 조건을 지정합니다. 이 제약 조건은 모니터링 시 모델의 성능을 검증하는 데 사용됩니다.\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(), # 모니터링 작업을 매시간 실행하도록 설정하는 크론 표현식을 생성합니다. \n",
    "    enable_cloudwatch_metrics=True, # CloudWatch 메트릭스를 활성화해서 기록합니다. \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c556ecf8",
   "metadata": {
    "papermill": {
     "duration": 0.049771,
     "end_time": "2022-04-18T00:29:48.825958",
     "exception": false,
     "start_time": "2022-04-18T00:29:48.776187",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Start generating some artificial traffic\n",
    "\n",
    "아래의 셀은 엔드포인트로 인위적인 트래픽을 보내기 위해 스레드를 시작합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23e81c77",
   "metadata": {
    "papermill": {
     "duration": 0.06214,
     "end_time": "2022-04-18T00:29:48.938258",
     "exception": false,
     "start_time": "2022-04-18T00:29:48.876118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "from time import sleep\n",
    "\n",
    "endpoint_name = predictor.endpoint\n",
    "runtime_client = sm_session.sagemaker_runtime_client\n",
    "\n",
    "# (just repeating code from above for convenience/ able to run this section independently)\n",
    "def invoke_endpoint(ep_name, file_name, runtime_client):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        for row in f:\n",
    "            payload = row.rstrip(\"\\n\")\n",
    "            response = runtime_client.invoke_endpoint(\n",
    "                EndpointName=ep_name, ContentType=\"text/csv\", Body=payload\n",
    "            )\n",
    "            response[\"Body\"].read()\n",
    "            time.sleep(1)\n",
    "\n",
    "\n",
    "def invoke_endpoint_forever():\n",
    "    while True:\n",
    "        try:\n",
    "            invoke_endpoint(endpoint_name, \"test_data/test-dataset-input-cols.csv\", runtime_client)\n",
    "        except runtime_client.exceptions.ValidationError:\n",
    "            pass\n",
    "\n",
    "\n",
    "thread = Thread(target=invoke_endpoint_forever)\n",
    "thread.start()\n",
    "\n",
    "# Note that you need to stop the kernel to stop the invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22785b69",
   "metadata": {
    "papermill": {
     "duration": 0.050394,
     "end_time": "2022-04-18T00:29:49.042351",
     "exception": false,
     "start_time": "2022-04-18T00:29:48.991957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Describe and inspect the schedule\n",
    "\n",
    "Once you describe, observe that the MonitoringScheduleStatus changes to Scheduled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d272369",
   "metadata": {
    "papermill": {
     "duration": 0.094778,
     "end_time": "2022-04-18T00:29:49.187187",
     "exception": false,
     "start_time": "2022-04-18T00:29:49.092409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule status: Scheduled\n"
     ]
    }
   ],
   "source": [
    "desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "print(\"Schedule status: {}\".format(desc_schedule_result[\"MonitoringScheduleStatus\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3fe71d",
   "metadata": {
    "papermill": {
     "duration": 0.050423,
     "end_time": "2022-04-18T00:29:49.289400",
     "exception": false,
     "start_time": "2022-04-18T00:29:49.238977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### List executions (실행 x)\n",
    "\n",
    "MonitoringScheduleStatus은 이전에 지정한 간격에 따라 작업을 시작합니다.\n",
    "실행을 확인하려면 UTC 시간 기준으로 한 시간이 경과할 때까지 기다려야 할 수 있습니다.\n",
    "Note: Even for an hourly schedule, Amazon SageMaker has a buffer period of 20 minutes to schedule your execution. You might see your execution start in anywhere from zero to ~20 minutes from the hour boundary. This is expected and done for load balancing in the backend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044bfcfa-443e-431a-a5bf-86f2798da8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_executions = my_default_monitor.list_executions()\n",
    "print(\n",
    "    \"We created a hourly schedule above that begins executions ON the hour (plus 0-20 min buffer.\\nWe will have to wait till we hit the hour...\"\n",
    ")\n",
    "\n",
    "while len(mon_executions) == 0:\n",
    "    print(\"Waiting for the first execution to happen...\")\n",
    "    time.sleep(60)\n",
    "    mon_executions = my_default_monitor.list_executions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708e5a91",
   "metadata": {
    "papermill": {
     "duration": 0.059309,
     "end_time": "2022-04-18T01:10:54.142040",
     "exception": false,
     "start_time": "2022-04-18T01:10:54.082731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Inspect a specific execution (latest execution)\n",
    "\n",
    "최신 완료된 또는 실패한 예약 실행을 확인했습니다. 여기서는 가능한 최종 상태와 각 상태의 의미를 설명합니다:\n",
    "\n",
    "- Completed - 모니터링 실행이 완료되었으며, 위반 보고서에서 문제를 발견하지 못한 상태입니다.\n",
    "- CompletedWithViolations - 모니터링 실행이 완료되었지만, 제약 조건 위반이 발견된 상태입니다.\n",
    "- Failed - 모니터링 실행이 실패했습니다. 이는 클라이언트 오류(예: 잘못된 역할 권한) 또는 인프라 문제 때문일 수 있습니다. FailureReason과 ExitMessage를 추가로 조사하여 정확한 원인을 파악해야 합니다.\n",
    "- Stopped - 작업이 최대 실행 시간을 초과했거나 수동으로 중지된 상태입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dac54e-46dd-4d3c-943b-35bca504024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_execution = mon_executions[-1]  # Latest execution's index is -1, second to last is -2, etc\n",
    "time.sleep(60)\n",
    "latest_execution.wait(logs=False)\n",
    "\n",
    "print(\"Latest execution status: {}\".format(latest_execution.describe()[\"ProcessingJobStatus\"]))\n",
    "print(\"Latest execution result: {}\".format(latest_execution.describe()[\"ExitMessage\"]))\n",
    "\n",
    "latest_job = latest_execution.describe()\n",
    "if latest_job[\"ProcessingJobStatus\"] != \"Completed\":\n",
    "    print(\n",
    "        \"====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34b857e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T01:15:40.445435Z",
     "iopub.status.busy": "2022-04-18T01:15:40.444884Z",
     "iopub.status.idle": "2022-04-18T01:15:40.447659Z",
     "shell.execute_reply": "2022-04-18T01:15:40.447208Z"
    },
    "papermill": {
     "duration": 0.074909,
     "end_time": "2022-04-18T01:15:40.447784",
     "exception": false,
     "start_time": "2022-04-18T01:15:40.372875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report Uri: s3://sagemaker-us-west-2-000000000000/sagemaker/DEMO-ModelMonitor/reports/DEMO-xgb-churn-pred-model-monitor-2022-04-18-00-13-16/DEMO-xgb-churn-pred-model-monitor-schedule-2022-04-18-00-29-48/2022/04/18/01\n"
     ]
    }
   ],
   "source": [
    "report_uri = latest_execution.output.destination\n",
    "print(\"Report Uri: {}\".format(report_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4070b1",
   "metadata": {
    "papermill": {
     "duration": 0.068521,
     "end_time": "2022-04-18T01:15:40.585134",
     "exception": false,
     "start_time": "2022-04-18T01:15:40.516613",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### List the generated reports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "227c215b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T01:15:40.736923Z",
     "iopub.status.busy": "2022-04-18T01:15:40.728273Z",
     "iopub.status.idle": "2022-04-18T01:15:40.987597Z",
     "shell.execute_reply": "2022-04-18T01:15:40.987996Z"
    },
    "papermill": {
     "duration": 0.33424,
     "end_time": "2022-04-18T01:15:40.988140",
     "exception": false,
     "start_time": "2022-04-18T01:15:40.653900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report bucket: sagemaker-us-west-2-521695447989\n",
      "Report key: sagemaker/DEMO-ModelMonitor/reports/DEMO-xgb-churn-pred-model-monitor-2022-04-18-00-13-16/DEMO-xgb-churn-pred-model-monitor-schedule-2022-04-18-00-29-48/2022/04/18/01\n",
      "Found Report Files:\n",
      "sagemaker/DEMO-ModelMonitor/reports/DEMO-xgb-churn-pred-model-monitor-2022-04-18-00-13-16/DEMO-xgb-churn-pred-model-monitor-schedule-2022-04-18-00-29-48/2022/04/18/01/constraint_violations.json\n",
      " sagemaker/DEMO-ModelMonitor/reports/DEMO-xgb-churn-pred-model-monitor-2022-04-18-00-13-16/DEMO-xgb-churn-pred-model-monitor-schedule-2022-04-18-00-29-48/2022/04/18/01/constraints.json\n",
      " sagemaker/DEMO-ModelMonitor/reports/DEMO-xgb-churn-pred-model-monitor-2022-04-18-00-13-16/DEMO-xgb-churn-pred-model-monitor-schedule-2022-04-18-00-29-48/2022/04/18/01/statistics.json\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "s3uri = urlparse(report_uri)\n",
    "report_bucket = s3uri.netloc\n",
    "report_key = s3uri.path.lstrip(\"/\")\n",
    "print(\"Report bucket: {}\".format(report_bucket))\n",
    "print(\"Report key: {}\".format(report_key))\n",
    "\n",
    "s3_client = boto3.Session().client(\"s3\")\n",
    "result = s3_client.list_objects(Bucket=report_bucket, Prefix=report_key)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get(\"Contents\")]\n",
    "print(\"Found Report Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38abe9e0",
   "metadata": {
    "papermill": {
     "duration": 0.069468,
     "end_time": "2022-04-18T01:15:41.127235",
     "exception": false,
     "start_time": "2022-04-18T01:15:41.057767",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Violations report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77403388",
   "metadata": {
    "papermill": {
     "duration": 0.070022,
     "end_time": "2022-04-18T01:15:41.266951",
     "exception": false,
     "start_time": "2022-04-18T01:15:41.196929",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Any violations compared to the baseline are listed below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "111944d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T01:15:41.411758Z",
     "iopub.status.busy": "2022-04-18T01:15:41.411186Z",
     "iopub.status.idle": "2022-04-18T01:15:41.543054Z",
     "shell.execute_reply": "2022-04-18T01:15:41.543448Z"
    },
    "papermill": {
     "duration": 0.206895,
     "end_time": "2022-04-18T01:15:41.543590",
     "exception": false,
     "start_time": "2022-04-18T01:15:41.336695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>constraint_check_type</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Area Code_408</td>\n",
       "      <td>data_type_check</td>\n",
       "      <td>Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.64592817400101% of data is Integral.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VMail Plan_no</td>\n",
       "      <td>data_type_check</td>\n",
       "      <td>Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.64592817400101% of data is Integral.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>State_NV</td>\n",
       "      <td>data_type_check</td>\n",
       "      <td>Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.64592817400101% of data is Integral.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>State_NC</td>\n",
       "      <td>data_type_check</td>\n",
       "      <td>Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.64592817400101% of data is Integral.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>State_SC</td>\n",
       "      <td>data_type_check</td>\n",
       "      <td>Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.64592817400101% of data is Integral.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>State_UT</td>\n",
       "      <td>data_type_check</td>\n",
       "      <td>Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.64592817400101% of data is Integral.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>State_LA</td>\n",
       "      <td>data_type_check</td>\n",
       "      <td>Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.64592817400101% of data is Integral.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>State_CA</td>\n",
       "      <td>data_type_check</td>\n",
       "      <td>Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.64592817400101% of data is Integral.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>State_RI</td>\n",
       "      <td>data_type_check</td>\n",
       "      <td>Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.64592817400101% of data is Integral.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Churn</td>\n",
       "      <td>data_type_check</td>\n",
       "      <td>Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 0.0% of data is Integral.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature_name constraint_check_type  \\\n",
       "0  Area Code_408       data_type_check   \n",
       "1  VMail Plan_no       data_type_check   \n",
       "2       State_NV       data_type_check   \n",
       "3       State_NC       data_type_check   \n",
       "4       State_SC       data_type_check   \n",
       "5       State_UT       data_type_check   \n",
       "6       State_LA       data_type_check   \n",
       "7       State_CA       data_type_check   \n",
       "8       State_RI       data_type_check   \n",
       "9          Churn       data_type_check   \n",
       "\n",
       "                                                                                                                                            description  \n",
       "0  Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.64592817400101% of data is Integral.  \n",
       "1  Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.64592817400101% of data is Integral.  \n",
       "2  Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.64592817400101% of data is Integral.  \n",
       "3  Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.64592817400101% of data is Integral.  \n",
       "4  Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.64592817400101% of data is Integral.  \n",
       "5  Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.64592817400101% of data is Integral.  \n",
       "6  Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.64592817400101% of data is Integral.  \n",
       "7  Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.64592817400101% of data is Integral.  \n",
       "8  Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.64592817400101% of data is Integral.  \n",
       "9                Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 0.0% of data is Integral.  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "violations = my_default_monitor.latest_monitoring_constraint_violations()\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "constraints_df = pd.io.json.json_normalize(violations.body_dict[\"violations\"])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de159706",
   "metadata": {
    "papermill": {
     "duration": 0.070228,
     "end_time": "2022-04-18T01:15:41.683659",
     "exception": false,
     "start_time": "2022-04-18T01:15:41.613431",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Other commands\n",
    "\n",
    "We can also start and stop the monitoring schedules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70749848",
   "metadata": {
    "papermill": {
     "duration": 0.075182,
     "end_time": "2022-04-18T01:15:41.829067",
     "exception": false,
     "start_time": "2022-04-18T01:15:41.753885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Stopping Monitoring Schedule with name: DEMO-xgb-churn-pred-model-monitor-schedule-2024-07-22-20-55-19\n"
     ]
    }
   ],
   "source": [
    "my_default_monitor.stop_monitoring_schedule()\n",
    "# my_default_monitor.start_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8713a6",
   "metadata": {
    "papermill": {
     "duration": 0.070556,
     "end_time": "2022-04-18T01:15:41.969642",
     "exception": false,
     "start_time": "2022-04-18T01:15:41.899086",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Delete resources\n",
    "\n",
    "You can keep your endpoint running to continue capturing data. If you do not plan to collect more data or use this endpoint further, delete the endpoint to avoid incurring additional charges. Note that deleting your endpoint does not delete the data that was captured during the model invocations. That data persists in Amazon S3 until you delete it yourself.\n",
    "\n",
    "You need to delete the schedule before deleting the model and endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1af984eb",
   "metadata": {
    "papermill": {
     "duration": 80.76217,
     "end_time": "2022-04-18T01:17:02.831829",
     "exception": false,
     "start_time": "2022-04-18T01:15:42.069659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Stopping Monitoring Schedule with name: DEMO-xgb-churn-pred-model-monitor-schedule-2024-07-22-20-55-19\n",
      "INFO:sagemaker:Deleting Monitoring Schedule with name: DEMO-xgb-churn-pred-model-monitor-schedule-2024-07-22-20-55-19\n",
      "INFO:sagemaker.model_monitor.model_monitoring:Deleting Data Quality Job Definition with name: data-quality-job-definition-2024-07-22-20-55-19-707\n"
     ]
    }
   ],
   "source": [
    "my_default_monitor.stop_monitoring_schedule()\n",
    "my_default_monitor.delete_monitoring_schedule()\n",
    "time.sleep(60)  # Wait for the deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5d93a04",
   "metadata": {
    "papermill": {
     "duration": 0.291003,
     "end_time": "2022-04-18T01:17:03.202589",
     "exception": false,
     "start_time": "2022-04-18T01:17:02.911586",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: sagemaker-xgboost-2024-07-22-20-26-07-910\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2fd0f570",
   "metadata": {
    "papermill": {
     "duration": 0.290093,
     "end_time": "2022-04-18T01:17:03.565886",
     "exception": false,
     "start_time": "2022-04-18T01:17:03.275793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: DEMO-xgb-churn-pred-model-monitor-2024-07-22-20-26-07\n",
      "INFO:sagemaker:Deleting endpoint with name: DEMO-xgb-churn-pred-model-monitor-2024-07-22-20-26-07\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635f5909-208c-4286-8c2e-291ff9d8e240",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "papermill": {
   "default_parameters": {},
   "duration": 3833.130815,
   "end_time": "2022-04-18T01:17:06.266500",
   "environment_variables": {},
   "exception": null,
   "input_path": "SageMaker-ModelMonitoring.ipynb",
   "output_path": "/opt/ml/processing/output/SageMaker-ModelMonitoring-2022-04-18-00-08-24.ipynb",
   "parameters": {
    "kms_key": "arn:aws:kms:us-west-2:000000000000:1234abcd-12ab-34cd-56ef-1234567890ab"
   },
   "start_time": "2022-04-18T00:13:13.135685",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
